{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplearning with Transformer Architectures\n",
    "\n",
    "(Explanation of method here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Imports and PIP Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /home/benjamin/.local/lib/python3.10/site-packages (4.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (0.30.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (4.50.3)\n",
      "Requirement already satisfied: scikit-learn in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: tqdm in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: scipy in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/benjamin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: requests in /home/benjamin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/benjamin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/benjamin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/benjamin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: jinja2 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/benjamin/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/benjamin/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/benjamin/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/benjamin/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/benjamin/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/benjamin/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/benjamin/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/benjamin/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/benjamin/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import all relevant modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BigBirdModel, BigBirdTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# CUDA debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class and Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessing\n",
    "\n",
    "def preprocess_line(line: str, params: set, tokenizer=None) -> list[str]:\n",
    "    '''\n",
    "    Preprocesses a line of text\n",
    "\n",
    "    :param line:\n",
    "    :param params:\n",
    "    :return:\n",
    "\n",
    "    Program flow:\n",
    "        If the line contains an email, trims out the email header\n",
    "        Tokenises the line\n",
    "        Applies various transformations\n",
    "            Removes stop words\n",
    "            Stems\n",
    "            Lemmatises\n",
    "            Remvoes non-alphanumeric characters\n",
    "            Sets all lowercase\n",
    "        Returns the list of tokens\n",
    "    '''\n",
    "\n",
    "    if \"trim email\" in params:\n",
    "        if \"-- Forwarded by\" in line:\n",
    "            before_email = line.split(\"-\")[0].strip()\n",
    "            email = \"-\" + line.split(\"-\", 1)[1] if \"-\" in line else \"\"\n",
    "            email_subject = email.split(\"Subject:\")[-1].strip() # if there's no subject, this keeps the whole email\n",
    "            line = before_email + \" \" + email_subject\n",
    "            line = line.strip()\n",
    "        line\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        tokens = nltk.tokenize.word_tokenize(line)\n",
    "    else:\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        starts_with_space_labels = [1 if \"Ġ\" in token else 0 for token in tokens]\n",
    "        tokens = [token.replace(\"Ġ\",\"\") if \"Ġ\" in token else token for token in tokens ]\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "\n",
    "    operations = {\n",
    "        \"stop words\": lambda tokens: [token if token.lower() not in nltk.corpus.stopwords.words('english') else \"\" for token in tokens],\n",
    "        \"stem\": lambda tokens: [nltk.PorterStemmer().stem(token) for token in tokens],\n",
    "        \"lemmatise\": lambda tokens: [nltk.WordNetLemmatizer().lemmatize(token) for token in tokens],\n",
    "        \"alphanumeric\": lambda tokens: [\"\".join(filter(str.isalnum, token)) for token in tokens],\n",
    "        \"lowercase\": lambda tokens: [token.lower() for token in tokens]\n",
    "    }\n",
    "\n",
    "    # Apply each operation if we define it in the params set\n",
    "    for key, action in operations.items():\n",
    "        if key in params:\n",
    "            tokens = action(tokens)\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        return [token for token in tokens if token != \"\"]\n",
    "    else:\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "        return [tokens[i] if not starts_with_space_labels[i] else \"\".join([\"Ġ\", tokens[i]]) for i in range(len(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and create dataset and data loader\n",
    "\n",
    "class reformattedDataset(Dataset):\n",
    "    def __init__(self, df_name, parameters, reset_cache=False):\n",
    "        # Name is in form <path>/<name>.csv\n",
    "        name = df_name.split(\"/\")[-1]\n",
    "        name = name.split(\".\")[0]\n",
    "        print(name)\n",
    "\n",
    "        df = pd.read_csv(df_name)\n",
    "        \n",
    "        if os.path.exists(f\"Cached_Roberta/{name}_output1_roberta.pt\") and os.path.exists(f\"Cached_Roberta/{name}_output2_roberta.pt\") and reset_cache == False:\n",
    "            print(\"Loading cached Roberta outputs...\")\n",
    "            loaded_o1 = torch.load(f\"Cached_Roberta/{name}_output1_roberta.pt\", weights_only=True)\n",
    "            self.output1_roberta = torch.stack([torch.tensor(t) for t in loaded_o1])\n",
    "            self.output2_roberta = torch.stack([torch.tensor(t) for t in torch.load(f\"Cached_Roberta/{name}_output2_roberta.pt\", weights_only=True)])\n",
    "            print(f\"Successfully loaded {len(self.output1_roberta)} Roberta outputs\")\n",
    "\n",
    "            self.labels = df[\"label\"][:len(self.output1_roberta)].reset_index(drop=True)\n",
    "\n",
    "        else:\n",
    "            tokenizer = BigBirdTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "\n",
    "            print(\"Preprocessing and converting tokens to strings...\")\n",
    "            texts1 = pd.Series([tokenizer.convert_tokens_to_string(preprocess_line(row,parameters,tokenizer)) for row in tqdm(df[\"text_1\"], desc=\"Processing text_1\", position=0, leave=True)])\n",
    "            texts2 = pd.Series([tokenizer.convert_tokens_to_string(preprocess_line(row,parameters,tokenizer)) for row in tqdm(df[\"text_2\"], desc=\"Processing text_2\", position=0, leave=True)])\n",
    "\n",
    "            print(\"Tokenising with BigBird\")\n",
    "            encodeds1 = pd.Series([tokenizer(text, return_tensors = \"pt\", padding=\"max_length\", max_length=514, truncation=True) for text in tqdm(texts1, desc=\"Tokenising text_1\", position=0, leave=True)])\n",
    "            encodeds2 = pd.Series([tokenizer(text, return_tensors = \"pt\", padding=\"max_length\", max_length=514, truncation=True) for text in tqdm(texts2, desc=\"Tokenising text-2\", position=0, leave=True)])\n",
    "\n",
    "            self.inputs1 = pd.Series([encoded[\"input_ids\"].squeeze(0) for encoded in encodeds1])\n",
    "            self.attention_masks1 = pd.Series([encoded[\"attention_mask\"].squeeze(0) for encoded in encodeds1])\n",
    "            self.inputs2 = pd.Series([encoded[\"input_ids\"].squeeze(0) for encoded in encodeds2])\n",
    "            self.attention_masks2 = pd.Series([encoded[\"attention_mask\"].squeeze(0) for encoded in encodeds2])\n",
    "\n",
    "            # pre compute roberta tensors to save time\n",
    "            print(\"Loading BigBird\")\n",
    "            self.roberta = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\").to(device)\n",
    "            for parameter in self.roberta.parameters():\n",
    "                parameter.requires_grad = False\n",
    "\n",
    "            print(\"Encoding with BigBird\")\n",
    "            outputs = []\n",
    "            self.roberta.eval()\n",
    "            batch_size = 16\n",
    "            with torch.no_grad():\n",
    "                for i in tqdm(range(0, len(self.inputs1), batch_size), desc=\"Encoding input1\", position=0, leave=True):\n",
    "                    batch_inputs = list(self.inputs1[i:i+batch_size])\n",
    "                    batch_masks = list(self.attention_masks1[i:i+batch_size])\n",
    "\n",
    "                    batch_inputs_tensor = torch.stack(batch_inputs).to(device)\n",
    "                    batch_masks_tensor = torch.stack(batch_masks).to(device)\n",
    "\n",
    "                    batch_outputs = self.roberta(batch_inputs_tensor, attention_mask=batch_masks_tensor).last_hidden_state[:, 0, :]\n",
    "\n",
    "                    outputs.extend(batch_outputs.cpu())\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            self.output1_roberta = torch.stack(outputs)\n",
    "            torch.save(self.output1_roberta.tolist(), f\"Cached_Roberta/{name}_output1_roberta.pt\")\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for i in tqdm(range(0, len(self.inputs2), batch_size), desc=\"Encoding input2\", position=0, leave=True):\n",
    "                    batch_inputs = list(self.inputs2[i:i+batch_size])\n",
    "                    batch_masks = list(self.attention_masks2[i:i+batch_size])\n",
    "\n",
    "                    batch_inputs_tensor = torch.stack(batch_inputs).to(device)\n",
    "                    batch_masks_tensor = torch.stack(batch_masks).to(device)\n",
    "\n",
    "                    batch_outputs = self.roberta(batch_inputs_tensor, attention_mask=batch_masks_tensor).last_hidden_state[:, 0, :]\n",
    "\n",
    "                    outputs.extend(batch_outputs.cpu())\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            self.output2_roberta = torch.stack(outputs)\n",
    "            torch.save(self.output2_roberta.tolist(), f\"Cached_Roberta/{name}_output2_roberta.pt\")\n",
    "            \n",
    "            # self.output1_roberta = torch.stack([self.roberta(input1.unsqueeze(0).to(device), attention_mask=attention_mask1.unsqueeze(0).to(device)).last_hidden_state[:, 0, :].squeeze(0).detach() for input1, attention_mask1 in tqdm(zip(self.inputs1, self.attention_masks1), total=len(self.inputs1), desc=\"Encoding input1\", position=0, leave=True)])\n",
    "            \n",
    "            \n",
    "            # self.output2_roberta = torch.stack([self.roberta(input2.unsqueeze(0).to(device), attention_mask=attention_mask2.unsqueeze(0).to(device)).last_hidden_state[:, 0, :].squeeze(0).detach() for input2, attention_mask2 in tqdm(zip(self.inputs2, self.attention_masks2), total=len(self.inputs2), desc=\"Encoding input2\", position=0, leave=True)])\n",
    "            # torch.save(self.output2_roberta.tolist(), \"Cached_Roberta/output2_roberta.pt\")\n",
    "            # save roberta tensors to file, prevents rerunning each time\n",
    "            \n",
    "\n",
    "\n",
    "            self.labels = df[\"label\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.output1_roberta)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        return self.output1_roberta[i], self.output2_roberta[i], torch.tensor(self.labels.iloc[i], dtype=torch.float)\n",
    "\n",
    "# dataset = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=False) #TODO try parameters\n",
    "# val_dataset = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=True)\n",
    "# test_dataset = reformattedDataset(\"Data/AV_trial.csv\", parameters={}, reset_cache=True)\n",
    "# dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our architecture\n",
    "\n",
    "class classifierRoberta(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classifierRoberta, self).__init__()\n",
    "\n",
    "        # load and freeze pretrained Roberta\n",
    "        self.roberta = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "        for parameter in self.roberta.parameters():\n",
    "            parameter.requires_grad = False \n",
    "        \n",
    "        # add our custom binary classifier layer to end\n",
    "        self.fc1 = nn.Linear(768*2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(self.fc1, self.fc2, self.fc3)\n",
    "        self.sigmoid = nn.Sigmoid() #to convert classifier output to probability\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        \n",
    "        # use pre computed Roberta tensors\n",
    "        combined_inputs = torch.cat((input1.squeeze(1), input2.squeeze(1)), dim=1)\n",
    "\n",
    "        # pass result through our appended classifier layers\n",
    "        classifier_output = self.classifier(combined_inputs)\n",
    "        return classifier_output\n",
    "        # probability = self.sigmoid(classifier_output)\n",
    "        # return probability.squeeze() #convert to 1D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier():\n",
    "    # Hyper-Parameters:\n",
    "    batch_size = 32\n",
    "    lr = 0.00001\n",
    "    epochs = 25000\n",
    "    threshold = 0\n",
    "    weight_decay = 0.01\n",
    "\n",
    "    model = classifierRoberta().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimiser = torch.optim.Adam(model.classifier.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    train_ds = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=False)\n",
    "    val_ds = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=False)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_list = []\n",
    "\n",
    "        for s1, s2, l in train_dl:\n",
    "            s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            output = model(s1, s2).reshape(l.shape[0])\n",
    "            loss = criterion(output, l)\n",
    "            loss_list.append(loss.detach().cpu().numpy())\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        print(f\"\\rMean training loss for epoch {epoch+1}: {np.mean(loss_list)}\", end=\"\")\n",
    "\n",
    "        # loss_list = []\n",
    "        # preds = []\n",
    "        # true_labels = []\n",
    "\n",
    "        # model.eval()\n",
    "        # with torch.no_grad():\n",
    "        #     for s1, s2, l in val_dl:\n",
    "        #         s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "        #         output = model(s1, s2).reshape(l.shape[0])\n",
    "        #         loss = criterion(output, l)\n",
    "        #         loss_list.append(loss.detach().cpu().numpy())\n",
    "        #         preds.append(1 if output[0] > threshold else 0)\n",
    "        #         true_labels.append(l.detach().cpu().numpy())\n",
    "\n",
    "        # correct = 0\n",
    "        # TPs, FPs, FNs, TNs = 0, 0, 0, 0\n",
    "        # for j in range(len(true_labels)):\n",
    "        #     if true_labels[j] == preds[j]:\n",
    "        #         correct += 1\n",
    "        #         if true_labels[j] == 1:\n",
    "        #             TPs += 1\n",
    "        #         else:\n",
    "        #             TNs += 1\n",
    "        #     else:\n",
    "        #         if true_labels[j] == 1:\n",
    "        #             FNs += 1\n",
    "        #         else:\n",
    "        #             FPs += 1\n",
    "\n",
    "        # accuracy = correct / len(true_labels)\n",
    "        # recall = 0 if TPs + FNs == 0 else TPs / (TPs + FNs)\n",
    "        # precision = 0 if TPs + FPs == 0 else TPs / (TPs + FPs)\n",
    "        # f1 = 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print()\n",
    "            loss_list = []\n",
    "            preds = []\n",
    "            true_labels = []\n",
    "    \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for s1, s2, l in val_dl:\n",
    "                    s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "                    output = model(s1, s2).reshape(l.shape[0])\n",
    "                    loss = criterion(output, l)\n",
    "                    loss_list.append(loss.detach().cpu().numpy())\n",
    "                    preds.append(1 if output[0] > threshold else 0)\n",
    "                    true_labels.append(l.detach().cpu().numpy())\n",
    "    \n",
    "            correct = 0\n",
    "            TPs, FPs, FNs, TNs = 0, 0, 0, 0\n",
    "            for j in range(len(true_labels)):\n",
    "                if true_labels[j] == preds[j]:\n",
    "                    correct += 1\n",
    "                    if true_labels[j] == 1:\n",
    "                        TPs += 1\n",
    "                    else:\n",
    "                        TNs += 1\n",
    "                else:\n",
    "                    if true_labels[j] == 1:\n",
    "                        FNs += 1\n",
    "                    else:\n",
    "                        FPs += 1\n",
    "    \n",
    "            accuracy = correct / len(true_labels)\n",
    "            recall = 0 if TPs + FNs == 0 else TPs / (TPs + FNs)\n",
    "            precision = 0 if TPs + FPs == 0 else TPs / (TPs + FPs)\n",
    "            f1 = 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "            print(f\"Testing Stats for Epoch {epoch+1}\")\n",
    "            print(f\"    Mean Loss: {np.mean(loss_list)}\")\n",
    "            print(f\"    Accuracy: {accuracy}\")\n",
    "            print(f\"    _____________________________\")\n",
    "            print(f\"    |True\\Pred|Positive|Negative|\")\n",
    "            print(f\"    |---------|--------|--------|\")\n",
    "            print(f\"    |Positive |TPs: {TPs}|FNs: {FNs}|\")\n",
    "            print(f\"    |---------|--------|--------|\")\n",
    "            print(f\"    |Negative |FPs: {FPs}|TNs: {TNs}|\")\n",
    "            print(f\"    |---------|--------|--------|\")\n",
    "            print(f\"    \")\n",
    "            print(f\"    Precision: {precision}\")\n",
    "            print(f\"    Recall: {recall}\")\n",
    "            print(f\"    F1 Score: {f1}\")\n",
    "\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model):\n",
    "    test_ds = reformattedDataset(\"Data/AV_trial.csv\", parameters={}, reset_cache=False)\n",
    "    test_dl = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    loss_list = []\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    threshold = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s1, s2, l in test_dl:\n",
    "            s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "\n",
    "            output = model(s1, s2).reshape(l.shape[0])\n",
    "            loss = criterion(output, l)\n",
    "            loss_list.append(loss.detach().cpu().numpy())\n",
    "            preds.append(1 if output[0] > threshold else 0)\n",
    "            true_labels.append(l.detach().cpu().numpy())\n",
    "\n",
    "    correct = 0\n",
    "    TPs = 0\n",
    "    FPs = 0\n",
    "    TNs = 0\n",
    "    FNs = 0\n",
    "\n",
    "    for i in range(len(true_labels)):\n",
    "        if true_labels[i] == preds[i]:\n",
    "            correct += 1\n",
    "            if true_labels[i] == 1:\n",
    "                TPs += 1\n",
    "            else:\n",
    "                TNs += 1\n",
    "        else:\n",
    "            if true_labels[i] == 1:\n",
    "                FNs += 1\n",
    "            else:\n",
    "                FPs += 1\n",
    "\n",
    "    accuracy = correct / len(true_labels)\n",
    "    recall = 0 if TPs + FNs == 0 else TPs / (TPs + FNs)\n",
    "    precision = 0 if TPs + FPs == 0 else TPs / (TPs + FPs)\n",
    "    f1 = 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Testing Stats for Classifier\")\n",
    "    print(f\"    Mean Loss: {np.mean(loss_list)}\")\n",
    "    print(f\"    Accuracy: {accuracy}\")\n",
    "    print(f\"    _____________________________\")\n",
    "    print(f\"    |True\\Pred|Positive|Negative|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    |Positive |TPs: {TPs}|FNs: {FNs}|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    |Negative |FPs: {FPs}|TNs: {TNs}|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    \")\n",
    "    print(f\"    Precision: {precision}\")\n",
    "    print(f\"    Recall: {recall}\")\n",
    "    print(f\"    F1 Score: {f1}\")\n",
    "\n",
    "    return (accuracy, recall, precision, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook \"Engine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook parameters - decides which of the next cells actually run\n",
    "generate_new_encodings = False\n",
    "train_new_classifier = True\n",
    "test_the_classifier = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset - run_next usually set to False as this means you can spam shift+enter to get through notebook\n",
    "if generate_new_encodings:\n",
    "    dataset = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=True) #TODO try parameters\n",
    "    val_dataset = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=True)\n",
    "    test_dataset = reformattedDataset(\"Data/test.csv\", parameters={}, reset_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Loading cached Roberta outputs...\n",
      "Successfully loaded 27643 Roberta outputs\n",
      "dev\n",
      "Loading cached Roberta outputs...\n",
      "Successfully loaded 5993 Roberta outputs\n",
      "Mean training loss for epoch 1: 0.6938681602478027\n",
      "Testing Stats for Epoch 1\n",
      "    Mean Loss: 0.6922438740730286\n",
      "    Accuracy: 0.5059235775070916\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 2993|FNs: 63|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 2898|TNs: 39|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.5080631471736547\n",
      "    Recall: 0.9793848167539267\n",
      "    F1 Score: 0.6690510785738237\n",
      "Mean training loss for epoch 143: 0.6931050419807434"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_new_classifier:\n\u001b[0;32m----> 2\u001b[0m     roberta_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(roberta_classifier\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCached_Roberta/trained_classifier_weights.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[105], line 29\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, l)\n\u001b[1;32m     28\u001b[0m     loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mMean training loss for epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(loss_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if train_new_classifier:\n",
    "    roberta_classifier = train_classifier()\n",
    "    torch.save(roberta_classifier.state_dict(), \"Cached_Roberta/trained_classifier_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AV_trial\n",
      "Loading cached Roberta outputs...\n",
      "Successfully loaded 50 Roberta outputs\n",
      "Testing Stats for Classifier\n",
      "    Mean Loss: 0.6931644678115845\n",
      "    Accuracy: 0.48\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 24|FNs: 1|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 25|TNs: 0|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.4897959183673469\n",
      "    Recall: 0.96\n",
      "    F1 Score: 0.6486486486486486\n"
     ]
    }
   ],
   "source": [
    "if test_the_classifier:\n",
    "    roberta_classifier = classifierRoberta().to(device)\n",
    "    roberta_classifier.load_state_dict(torch.load(\"Cached_Roberta/trained_classifier_weights.pt\", weights_only=True))\n",
    "    test_classifier(roberta_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m---> 14\u001b[0m     prediction, label \u001b[38;5;241m=\u001b[39m \u001b[43mmakePrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m>\u001b[39m T \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m     16\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend((prediction,\u001b[38;5;28mint\u001b[39m(label)))\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36mmakePrediction\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmakePrediction\u001b[39m(index):\n\u001b[1;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m dataset[index]\n\u001b[0;32m----> 5\u001b[0m     input_1 \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m     input_2 \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     label \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def makePrediction(index):\n",
    "    data = dataset[index]\n",
    "    input_1 = data[\"input1\"].to(device)\n",
    "    input_2 = data[\"input2\"].to(device)\n",
    "    label = data[\"label\"].to(device)\n",
    "\n",
    "    prediction = model(input_1.unsqueeze(0), input_2.unsqueeze(0))\n",
    "    return prediction.item(), label.item()\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(dataset)):\n",
    "    prediction, label = makePrediction(i)\n",
    "    prediction = 1 if prediction > T else 0 \n",
    "    predictions.append((prediction,int(label)))\n",
    "    print(predictions[i])\n",
    "\n",
    "correct = sum(pred == label for pred, label in predictions)\n",
    "accuracy = correct / len(predictions)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
