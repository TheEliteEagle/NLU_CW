{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplearning with Transformer Architectures\n",
    "\n",
    "(Explanation of method here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import all relevant modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BigBirdModel, BigBirdTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CUDA debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessing\n",
    "\n",
    "def preprocess_line(line: str, params: set, tokenizer=None) -> list[str]:\n",
    "    '''\n",
    "    Preprocesses a line of text\n",
    "\n",
    "    :param line:\n",
    "    :param params:\n",
    "    :return:\n",
    "\n",
    "    Program flow:\n",
    "        If the line contains an email, trims out the email header\n",
    "        Tokenises the line\n",
    "        Applies various transformations\n",
    "            Removes stop words\n",
    "            Stems\n",
    "            Lemmatises\n",
    "            Remvoes non-alphanumeric characters\n",
    "            Sets all lowercase\n",
    "        Returns the list of tokens\n",
    "    '''\n",
    "\n",
    "    if \"trim email\" in params:\n",
    "        if \"-- Forwarded by\" in line:\n",
    "            before_email = line.split(\"-\")[0].strip()\n",
    "            email = \"-\" + line.split(\"-\", 1)[1] if \"-\" in line else \"\"\n",
    "            email_subject = email.split(\"Subject:\")[-1].strip() # if there's no subject, this keeps the whole email\n",
    "            line = before_email + \" \" + email_subject\n",
    "            line = line.strip()\n",
    "        line\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        tokens = nltk.tokenize.word_tokenize(line)\n",
    "    else:\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        starts_with_space_labels = [1 if \"Ġ\" in token else 0 for token in tokens]\n",
    "        tokens = [token.replace(\"Ġ\",\"\") if \"Ġ\" in token else token for token in tokens ]\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "\n",
    "    operations = {\n",
    "        \"stop words\": lambda tokens: [token if token.lower() not in nltk.corpus.stopwords.words('english') else \"\" for token in tokens],\n",
    "        \"stem\": lambda tokens: [nltk.PorterStemmer().stem(token) for token in tokens],\n",
    "        \"lemmatise\": lambda tokens: [nltk.WordNetLemmatizer().lemmatize(token) for token in tokens],\n",
    "        \"alphanumeric\": lambda tokens: [\"\".join(filter(str.isalnum, token)) for token in tokens],\n",
    "        \"lowercase\": lambda tokens: [token.lower() for token in tokens]\n",
    "    }\n",
    "\n",
    "    # Apply each operation if we define it in the params set\n",
    "    for key, action in operations.items():\n",
    "        if key in params:\n",
    "            tokens = action(tokens)\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        return [token for token in tokens if token != \"\"]\n",
    "    else:\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "        return [tokens[i] if not starts_with_space_labels[i] else \"\".join([\"Ġ\", tokens[i]]) for i in range(len(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Loading cached Roberta outputs...\n",
      "Successfully loaded 100 Roberta outputs\n",
      "dev\n",
      "Loading cached Roberta outputs...\n",
      "Successfully loaded 100 Roberta outputs\n"
     ]
    }
   ],
   "source": [
    "# Define and create dataset and data loader\n",
    "\n",
    "class reformattedDataset(Dataset):\n",
    "    def __init__(self, df_name, parameters, reset_cache=False):\n",
    "        # Name is in form <path>/<name>.csv\n",
    "        name = df_name.split(\"/\")[-1]\n",
    "        name = name.split(\".\")[0]\n",
    "        print(name)\n",
    "\n",
    "        df = pd.read_csv(df_name, nrows=100)\n",
    "        \n",
    "        if os.path.exists(f\"Cached_Roberta/{name}_output1_roberta.pt\") and os.path.exists(f\"Cached_Roberta/{name}_output2_roberta.pt\") and reset_cache == False:\n",
    "            print(\"Loading cached Roberta outputs...\")\n",
    "            loaded_o1 = torch.load(f\"Cached_Roberta/{name}_output1_roberta.pt\", weights_only=True)\n",
    "            self.output1_roberta = torch.stack([torch.tensor(t) for t in loaded_o1])\n",
    "            self.output2_roberta = torch.stack([torch.tensor(t) for t in torch.load(f\"Cached_Roberta/{name}_output2_roberta.pt\", weights_only=True)])\n",
    "            print(f\"Successfully loaded {len(self.output1_roberta)} Roberta outputs\")\n",
    "\n",
    "            self.labels = df[\"label\"][:len(self.output1_roberta)].reset_index(drop=True)\n",
    "\n",
    "        else:\n",
    "            tokenizer = BigBirdTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "\n",
    "            print(\"Preprocessing and converting tokens to strings...\")\n",
    "            texts1 = pd.Series([tokenizer.convert_tokens_to_string(preprocess_line(row,parameters,tokenizer)) for row in tqdm(df[\"text_1\"], desc=\"Processing text_1\", position=0, leave=True)])\n",
    "            texts2 = pd.Series([tokenizer.convert_tokens_to_string(preprocess_line(row,parameters,tokenizer)) for row in tqdm(df[\"text_2\"], desc=\"Processing text_2\", position=0, leave=True)])\n",
    "\n",
    "            print(\"Tokenising with BigBird\")\n",
    "            encodeds1 = pd.Series([tokenizer(text, return_tensors = \"pt\", padding=\"max_length\", max_length=514, truncation=True) for text in tqdm(texts1, desc=\"Tokenising text_1\", position=0, leave=True)])\n",
    "            encodeds2 = pd.Series([tokenizer(text, return_tensors = \"pt\", padding=\"max_length\", max_length=514, truncation=True) for text in tqdm(texts2, desc=\"Tokenising text-2\", position=0, leave=True)])\n",
    "\n",
    "            self.inputs1 = pd.Series([encoded[\"input_ids\"].squeeze(0) for encoded in encodeds1])\n",
    "            self.attention_masks1 = pd.Series([encoded[\"attention_mask\"].squeeze(0) for encoded in encodeds1])\n",
    "            self.inputs2 = pd.Series([encoded[\"input_ids\"].squeeze(0) for encoded in encodeds2])\n",
    "            self.attention_masks2 = pd.Series([encoded[\"attention_mask\"].squeeze(0) for encoded in encodeds2])\n",
    "\n",
    "            # pre compute roberta tensors to save time\n",
    "            print(\"Loading BigBird\")\n",
    "            self.roberta = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\").to(device)\n",
    "            for parameter in self.roberta.parameters():\n",
    "                parameter.requires_grad = False\n",
    "\n",
    "            print(\"Encoding with BigBird\")\n",
    "            outputs = []\n",
    "            self.roberta.eval()\n",
    "            batch_size = 16\n",
    "            with torch.no_grad():\n",
    "                for i in tqdm(range(0, len(self.inputs1), batch_size), desc=\"Encoding input1\", position=0, leave=True):\n",
    "                    batch_inputs = list(self.inputs1[i:i+batch_size])\n",
    "                    batch_masks = list(self.attention_masks1[i:i+batch_size])\n",
    "\n",
    "                    batch_inputs_tensor = torch.stack(batch_inputs).to(device)\n",
    "                    batch_masks_tensor = torch.stack(batch_masks).to(device)\n",
    "\n",
    "                    batch_outputs = self.roberta(batch_inputs_tensor, attention_mask=batch_masks_tensor).last_hidden_state[:, 0, :]\n",
    "\n",
    "                    outputs.extend(batch_outputs.cpu())\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            self.output1_roberta = torch.stack(outputs)\n",
    "            torch.save(self.output1_roberta.tolist(), f\"Cached_Roberta/{name}_output1_roberta.pt\")\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for i in tqdm(range(0, len(self.inputs2), batch_size), desc=\"Encoding input2\", position=0, leave=True):\n",
    "                    batch_inputs = list(self.inputs2[i:i+batch_size])\n",
    "                    batch_masks = list(self.attention_masks2[i:i+batch_size])\n",
    "\n",
    "                    batch_inputs_tensor = torch.stack(batch_inputs).to(device)\n",
    "                    batch_masks_tensor = torch.stack(batch_masks).to(device)\n",
    "\n",
    "                    batch_outputs = self.roberta(batch_inputs_tensor, attention_mask=batch_masks_tensor).last_hidden_state[:, 0, :]\n",
    "\n",
    "                    outputs.extend(batch_outputs.cpu())\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            self.output2_roberta = torch.stack(outputs)\n",
    "            torch.save(self.output2_roberta.tolist(), f\"Cached_Roberta/{name}_output2_roberta.pt\")\n",
    "            \n",
    "            # self.output1_roberta = torch.stack([self.roberta(input1.unsqueeze(0).to(device), attention_mask=attention_mask1.unsqueeze(0).to(device)).last_hidden_state[:, 0, :].squeeze(0).detach() for input1, attention_mask1 in tqdm(zip(self.inputs1, self.attention_masks1), total=len(self.inputs1), desc=\"Encoding input1\", position=0, leave=True)])\n",
    "            \n",
    "            \n",
    "            # self.output2_roberta = torch.stack([self.roberta(input2.unsqueeze(0).to(device), attention_mask=attention_mask2.unsqueeze(0).to(device)).last_hidden_state[:, 0, :].squeeze(0).detach() for input2, attention_mask2 in tqdm(zip(self.inputs2, self.attention_masks2), total=len(self.inputs2), desc=\"Encoding input2\", position=0, leave=True)])\n",
    "            # torch.save(self.output2_roberta.tolist(), \"Cached_Roberta/output2_roberta.pt\")\n",
    "            # save roberta tensors to file, prevents rerunning each time\n",
    "            \n",
    "\n",
    "\n",
    "            self.labels = df[\"label\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.output1_roberta)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        return self.output1_roberta[i], self.output2_roberta[i], torch.tensor(self.labels.iloc[i], dtype=torch.float)\n",
    "\n",
    "dataset = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=False) #TODO try parameters\n",
    "val_dataset = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=False)\n",
    "test_dataset = reformattedDataset(\"Data/AV_trial.csv\", parameters={}, reset_cach=True)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our architecture\n",
    "\n",
    "class classifierRoberta(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classifierRoberta, self).__init__()\n",
    "\n",
    "        # load and freeze pretrained Roberta\n",
    "        self.roberta = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "        for parameter in self.roberta.parameters():\n",
    "            parameter.requires_grad = False \n",
    "        \n",
    "        # add our custom binary classifier layer to end\n",
    "        self.fc1 = nn.Linear(768*2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(self.fc1, self.fc2, self.fc3)\n",
    "        self.sigmoid = nn.Sigmoid() #to convert classifier output to probability\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        \n",
    "        # use pre computed Roberta tensors\n",
    "        combined_inputs = torch.cat((input1.squeeze(1), input2.squeeze(1)), dim=1)\n",
    "\n",
    "        # pass result through our appended classifier layers\n",
    "        classifier_output = self.classifier(combined_inputs)\n",
    "        return classifier_output.squeeze()\n",
    "        # probability = self.sigmoid(classifier_output)\n",
    "        # return probability.squeeze() #convert to 1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model and Hyperparams\n",
    "model = classifierRoberta().to(device)\n",
    "optimiser = torch.optim.AdamW(model.classifier.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "loss_model = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.6956040263175964  Epoch: 0 \n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "epochs = 25\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for s1, s2, l in dataloader:\n",
    "        s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        predictions = model(s1, s2)\n",
    "        \n",
    "        loss = loss_model(predictions, l)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Loss {total_loss/len(dataloader)}  Epoch: {epoch} \") \n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"Cached_Roberta/trained_classifier_weights.pt\")\n",
    "\n",
    "# load model\n",
    "# model = classifierRoberta().to(device)\n",
    "# model.load_state_dict(torch.load(\"Cached_Roberta/trained_classifier_weights.pt\", weights_only=True))\n",
    "# model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO find optimal threshhold after training\n",
    "\n",
    "T = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model):\n",
    "    test_data = torch.load("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m---> 14\u001b[0m     prediction, label \u001b[38;5;241m=\u001b[39m \u001b[43mmakePrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m>\u001b[39m T \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m     16\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend((prediction,\u001b[38;5;28mint\u001b[39m(label)))\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36mmakePrediction\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmakePrediction\u001b[39m(index):\n\u001b[1;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m dataset[index]\n\u001b[0;32m----> 5\u001b[0m     input_1 \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m     input_2 \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     label \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def makePrediction(index):\n",
    "    data = dataset[index]\n",
    "    input_1 = data[\"input1\"].to(device)\n",
    "    input_2 = data[\"input2\"].to(device)\n",
    "    label = data[\"label\"].to(device)\n",
    "\n",
    "    prediction = model(input_1.unsqueeze(0), input_2.unsqueeze(0))\n",
    "    return prediction.item(), label.item()\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(dataset)):\n",
    "    prediction, label = makePrediction(i)\n",
    "    prediction = 1 if prediction > T else 0 \n",
    "    predictions.append((prediction,int(label)))\n",
    "    print(predictions[i])\n",
    "\n",
    "correct = sum(pred == label for pred, label in predictions)\n",
    "accuracy = correct / len(predictions)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
