{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplearning with Transformer Architectures\n",
    "\n",
    "There are three sections to this implementation.\n",
    "- Relevant Imports and PIP Installs (self explanatory)\n",
    "- Dataset class and preprocessing function. Creates and stores the mpnet embeddings on the pre-processed dataset.\n",
    "- Classifier Definition. Defines the ensemble as well as the binary classifier members.\n",
    "- Functions for training and testing.\n",
    "- The Notebook \"Engine\", which controls which cells will execute. For example sometimes we might want to skip training or eval etc.\n",
    "- Demo block (This requires running the previous cells to define the needed functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Imports and PIP Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.48.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.6.0+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.26.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: unknown command \"inst\" - maybe you meant \"list\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentence-transformers\n",
    "!pip3 inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\boltm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\boltm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x226330a1af0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all relevant modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class and Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessing\n",
    "\n",
    "def preprocess_line(line: str, params: set, tokenizer=None) -> list[str]:\n",
    "    '''\n",
    "    Preprocesses a line of text\n",
    "\n",
    "    :param line:\n",
    "    :param params:\n",
    "    :return:\n",
    "\n",
    "    Program flow:\n",
    "        If the line contains an email, trims out the email header\n",
    "        Tokenises the line\n",
    "        Applies various transformations\n",
    "            Removes stop words\n",
    "            Stems\n",
    "            Lemmatises\n",
    "            Remvoes non-alphanumeric characters\n",
    "            Sets all lowercase\n",
    "        Returns the list of tokens\n",
    "    '''\n",
    "\n",
    "    if \"trim email\" in params:\n",
    "        if \"-- Forwarded by\" in line:\n",
    "            before_email = line.split(\"-\")[0].strip()\n",
    "            email = \"-\" + line.split(\"-\", 1)[1] if \"-\" in line else \"\"\n",
    "            email_subject = email.split(\"Subject:\")[-1].strip() # if there's no subject, this keeps the whole email\n",
    "            line = before_email + \" \" + email_subject\n",
    "            line = line.strip()\n",
    "        line\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        tokens = nltk.tokenize.word_tokenize(line)\n",
    "    else:\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        starts_with_space_labels = [1 if \"Ġ\" in token else 0 for token in tokens]\n",
    "        tokens = [token.replace(\"Ġ\",\"\") if \"Ġ\" in token else token for token in tokens ]\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "\n",
    "    operations = {\n",
    "        \"stop words\": lambda tokens: [token if token.lower() not in nltk.corpus.stopwords.words('english') else \"\" for token in tokens],\n",
    "        \"stem\": lambda tokens: [nltk.PorterStemmer().stem(token) for token in tokens],\n",
    "        \"lemmatise\": lambda tokens: [nltk.WordNetLemmatizer().lemmatize(token) for token in tokens],\n",
    "        \"alphanumeric\": lambda tokens: [\"\".join(filter(str.isalnum, token)) for token in tokens],\n",
    "        \"lowercase\": lambda tokens: [token.lower() for token in tokens]\n",
    "    }\n",
    "\n",
    "    # Apply each operation if we define it in the params set\n",
    "    for key, action in operations.items():\n",
    "        if key in params:\n",
    "            tokens = action(tokens)\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        return [token for token in tokens if token != \"\"]\n",
    "    else:\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "        return [tokens[i] if not starts_with_space_labels[i] else \"\".join([\"Ġ\", tokens[i]]) for i in range(len(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and create dataset and data loader\n",
    "\n",
    "class reformattedDataset(Dataset):\n",
    "    def __init__(self, df_name, parameters, reset_cache=False):\n",
    "        # Name is in form <path>/<name>.csv\n",
    "        name = df_name.split(\"/\")[-1]\n",
    "        name = name.split(\".\")[0]\n",
    "        print(name)\n",
    "\n",
    "        df = pd.read_csv(df_name)\n",
    "        \n",
    "        # load stores embeddings if found\n",
    "        if os.path.exists(f\"Cached_MPNet/{name}_output1.pt\") and os.path.exists(f\"Cached_MPNet/{name}_output2.pt\") and reset_cache == False:\n",
    "            print(\"Loading cached MPNet outputs...\")\n",
    "            loaded_o1 = torch.load(f\"Cached_MPNet/{name}_output1.pt\", weights_only=True, map_location=device)\n",
    "            self.output1 = torch.stack([torch.tensor(t) for t in loaded_o1])\n",
    "            self.output2 = torch.stack([torch.tensor(t) for t in torch.load(f\"Cached_MPNet/{name}_output2.pt\", weights_only=True, map_location=device)])\n",
    "            print(f\"Successfully loaded {len(self.output1)} MPNet outputs\")\n",
    "\n",
    "            self.labels = df[\"label\"][:len(self.output1)].reset_index(drop=True)\n",
    "\n",
    "        # create embeddings if not found\n",
    "        else:\n",
    "            mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "            texts1 = df[\"text_1\"].astype(str).tolist()\n",
    "            texts2 = df[\"text_2\"].astype(str).tolist()\n",
    "\n",
    "            self.output1 = torch.tensor(mpnet.encode(texts1, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            self.output2 = torch.tensor(mpnet.encode(texts2, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            \n",
    "            self.labels = df[\"label\"]\n",
    "\n",
    "            torch.save(self.output1, f\"Cached_MPNet/{name}_output1.pt\")\n",
    "            torch.save(self.output2, f\"Cached_MPNet/{name}_output2.pt\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.output1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        return self.output1[i], self.output2[i], torch.tensor(self.labels.iloc[i], dtype=torch.float)\n",
    "\n",
    "# same as above class, except no label\n",
    "class MPNetTestDataset(Dataset):\n",
    "    def __init__(self, df_name, parameters, reset_cache=False):\n",
    "        # Name is in form <path>/<name>.csv\n",
    "        name = df_name.split(\"/\")[-1]\n",
    "        name = name.split(\".\")[0]\n",
    "        print(name)\n",
    "\n",
    "        df = pd.read_csv(df_name)\n",
    "        \n",
    "        if os.path.exists(f\"Cached_MPNet/{name}_output1.pt\") and os.path.exists(f\"Cached_MPNet/{name}_output2.pt\") and reset_cache == False:\n",
    "            print(\"Loading cached MPNet outputs...\")\n",
    "            loaded_o1 = torch.load(f\"Cached_MPNet/{name}_output1.pt\", weights_only=True, map_location=device)\n",
    "            self.output1 = torch.stack([torch.tensor(t) for t in loaded_o1])\n",
    "            self.output2 = torch.stack([torch.tensor(t) for t in torch.load(f\"Cached_MPNet/{name}_output2.pt\", weights_only=True, map_location=device)])\n",
    "            print(f\"Successfully loaded {len(self.output1)} MPNet outputs\")\n",
    "\n",
    "        else:\n",
    "            mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "            texts1 = df[\"text_1\"].astype(str).tolist()\n",
    "            texts2 = df[\"text_2\"].astype(str).tolist()\n",
    "\n",
    "            self.output1 = torch.tensor(mpnet.encode(texts1, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            self.output2 = torch.tensor(mpnet.encode(texts2, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            \n",
    "            torch.save(self.output1, f\"Cached_MPNet/{name}_output1.pt\")\n",
    "            torch.save(self.output2, f\"Cached_MPNet/{name}_output2.pt\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.output1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        return self.output1[i], self.output2[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our architecture\n",
    "\n",
    "class classifierMPNet(nn.Module):\n",
    "    def __init__(self,child_num):\n",
    "        super(classifierMPNet, self).__init__()\n",
    "\n",
    "        self.child_num = child_num\n",
    "\n",
    "        # add our custom binary classifier layer to end\n",
    "        self.fc1 = nn.Linear(768*4 + 1, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            self.fc1,\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.Dropout(0.3),\n",
    "            self.fc2, \n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.3),\n",
    "            self.fc3\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid() #to convert classifier output to probability\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # use pre computed MPNet tensors\n",
    "        cos_sim = nn.functional.cosine_similarity(input1, input2, dim=1).unsqueeze(1)\n",
    "        combined_inputs = torch.cat((input1, input2, torch.abs(input1-input2), input1*input2, cos_sim), dim=1)\n",
    "\n",
    "        # pass result through our appended classifier layers\n",
    "        classifier_output = self.classifier(combined_inputs)\n",
    "        return classifier_output\n",
    "\n",
    "\n",
    "class AverageClassifier(nn.Module):\n",
    "    def __init__(self, children=5):\n",
    "        super(AverageClassifier, self).__init__()\n",
    "\n",
    "        # create ensemble members\n",
    "        self.clfs = nn.ModuleList([classifierMPNet(child_num) for child_num in range(children)])\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        outs = [clf(input1, input2) for clf in self.clfs]\n",
    "        stack = torch.stack(outs, dim=0)\n",
    "        return torch.mean(stack, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier():\n",
    "    # Hyper-Parameters:\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "    epochs = 40\n",
    "    threshold = 0\n",
    "    weight_decay = 0.01\n",
    "\n",
    "    num_children = 5\n",
    "    model = AverageClassifier(num_children).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    train_ds = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=False)\n",
    "    val_ds = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=False)\n",
    "    val_dl = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # split training dataset into subsets, one for each child\n",
    "    subset_ratio = 0.8\n",
    "    train_ds_splits = []\n",
    "    for _ in range(num_children):\n",
    "        indices = np.random.choice(len(train_ds), int(len(train_ds) * subset_ratio), replace=True)\n",
    "        subset = Subset(train_ds, indices)\n",
    "        train_ds_splits.append(subset)\n",
    "    \n",
    "    # train each child on its own data split\n",
    "    for child_num, child_model in enumerate(model.clfs):\n",
    "        print(f\"Training child #{child_num + 1}\")\n",
    "\n",
    "        optimiser = torch.optim.AdamW(child_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        child_dl = DataLoader(train_ds_splits[child_num], batch_size=batch_size, shuffle=True)\n",
    "        for epoch in range(epochs):\n",
    "            child_model.train()\n",
    "            loss_list = []\n",
    "\n",
    "            for s1, s2, l in child_dl:\n",
    "                s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "\n",
    "                optimiser.zero_grad()\n",
    "                output = child_model(s1, s2).reshape(l.shape[0]) \n",
    "                loss = criterion(output, l)\n",
    "                loss_list.append(loss.detach().cpu().numpy()) \n",
    "                loss.backward() \n",
    "                optimiser.step()\n",
    "            \n",
    "            print(f\"Child: {child_num+1}, Epoch {epoch+1}, Loss: {np.mean(loss_list)}\")\n",
    "\n",
    "        # eval the child\n",
    "        loss_list = []\n",
    "        preds = []\n",
    "        true_labels = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for s1, s2, l in val_dl:\n",
    "                s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "                output = model(s1, s2).reshape(l.shape[0])\n",
    "                loss = criterion(output, l)\n",
    "                loss_list.append(loss.detach().cpu().numpy())\n",
    "                preds.append(1 if output[0] > threshold else 0)\n",
    "                true_labels.append(l.detach().cpu().numpy())\n",
    "\n",
    "        correct = 0\n",
    "        TPs, FPs, FNs, TNs = 0, 0, 0, 0\n",
    "        for j in range(len(true_labels)):\n",
    "            if true_labels[j] == preds[j]:\n",
    "                correct += 1\n",
    "                if true_labels[j] == 1:\n",
    "                    TPs += 1\n",
    "                else:\n",
    "                    TNs += 1\n",
    "            else:\n",
    "                if true_labels[j] == 1:\n",
    "                    FNs += 1\n",
    "                else:\n",
    "                    FPs += 1\n",
    "\n",
    "        accuracy = correct / len(true_labels)\n",
    "        recall = 0 if TPs + FNs == 0 else TPs / (TPs + FNs)\n",
    "        precision = 0 if TPs + FPs == 0 else TPs / (TPs + FPs)\n",
    "        f1 = 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        print(f\"Testing Stats for Epoch {epoch+1}\")\n",
    "        print(f\"    Mean Loss: {np.mean(loss_list)}\")\n",
    "        print(f\"    Accuracy: {accuracy}\")\n",
    "        print(f\"    _____________________________\")\n",
    "        print(f\"    |True\\Pred|Positive|Negative|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    |Positive |TPs: {TPs}|FNs: {FNs}|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    |Negative |FPs: {FPs}|TNs: {TNs}|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    \")\n",
    "        print(f\"    Precision: {precision}\")\n",
    "        print(f\"    Recall: {recall}\")\n",
    "        print(f\"    F1 Score: {f1}\")\n",
    "\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a finished trained model\n",
    "def eval_classifier(model):\n",
    "    val_ds = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=False)\n",
    "    val_dl = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    threshold = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for s1, s2, l in val_dl:\n",
    "            s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "            output = model(s1, s2).reshape(l.shape[0])\n",
    "            loss = criterion(output, l)\n",
    "            loss_list.append(loss.detach().cpu().numpy())\n",
    "            preds.append(1 if output[0] > threshold else 0)\n",
    "            true_labels.append(l.detach().cpu().numpy())\n",
    "\n",
    "    correct = 0\n",
    "    TPs, FPs, FNs, TNs = 0, 0, 0, 0\n",
    "    for j in range(len(true_labels)):\n",
    "        if true_labels[j] == preds[j]:\n",
    "            correct += 1\n",
    "            if true_labels[j] == 1:\n",
    "                TPs += 1\n",
    "            else:\n",
    "                TNs += 1\n",
    "        else:\n",
    "            if true_labels[j] == 1:\n",
    "                FNs += 1\n",
    "            else:\n",
    "                FPs += 1\n",
    "\n",
    "    accuracy = correct / len(true_labels)\n",
    "    recall = 0 if TPs + FNs == 0 else TPs / (TPs + FNs)\n",
    "    precision = 0 if TPs + FPs == 0 else TPs / (TPs + FPs)\n",
    "    f1 = 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Testing Stats for MPNet Classifer (Threshold: {threshold})\")\n",
    "    print(f\"    Mean Loss: {np.mean(loss_list)}\")\n",
    "    print(f\"    Accuracy: {accuracy}\")\n",
    "    print(f\"    _____________________________\")\n",
    "    print(f\"    |True\\Pred|Positive|Negative|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    |Positive |TPs: {TPs}|FNs: {FNs}|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    |Negative |FPs: {FPs}|TNs: {TNs}|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    \")\n",
    "    print(f\"    Precision: {precision}\")\n",
    "    print(f\"    Recall: {recall}\")\n",
    "    print(f\"    F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model, data_path):\n",
    "    test_ds = MPNetTestDataset(data_path, parameters={}, reset_cache=False)\n",
    "    test_dl = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    loss_list = []\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    threshold = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s1, s2 in test_dl:\n",
    "            s1, s2 = s1.to(device), s2.to(device)\n",
    "\n",
    "            output = model(s1, s2)\n",
    "            preds.append(1 if output[0] > threshold else 0)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook \"Engine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook parameters - decides which of the next cells actually run\n",
    "generate_new_encodings = False\n",
    "train_new_classifier = False\n",
    "test_the_classifier = False\n",
    "generate_evaluation_csv = False\n",
    "evaluate_classifier = False\n",
    "\n",
    "test_data_path = \"Data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset - run_next usually set to False as this means you can spam shift+enter to get through notebook\n",
    "if generate_new_encodings:\n",
    "    dataset = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=True)\n",
    "    val_dataset = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 27643 MPNet outputs\n",
      "dev\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 5993 MPNet outputs\n",
      "Training child #1\n",
      "Child: 1, Epoch 1, Loss: 0.7068110108375549\n",
      "Child: 1, Epoch 2, Loss: 0.6446200013160706\n",
      "Child: 1, Epoch 3, Loss: 0.5968915224075317\n",
      "Child: 1, Epoch 4, Loss: 0.5501615405082703\n",
      "Child: 1, Epoch 5, Loss: 0.49667781591415405\n",
      "Child: 1, Epoch 6, Loss: 0.4278343915939331\n",
      "Child: 1, Epoch 7, Loss: 0.359526664018631\n",
      "Child: 1, Epoch 8, Loss: 0.29951244592666626\n",
      "Child: 1, Epoch 9, Loss: 0.2529545724391937\n",
      "Child: 1, Epoch 10, Loss: 0.20428334176540375\n",
      "Child: 1, Epoch 11, Loss: 0.17147284746170044\n",
      "Child: 1, Epoch 12, Loss: 0.1475798785686493\n",
      "Child: 1, Epoch 13, Loss: 0.12940286099910736\n",
      "Child: 1, Epoch 14, Loss: 0.11391938477754593\n",
      "Child: 1, Epoch 15, Loss: 0.09809981286525726\n",
      "Child: 1, Epoch 16, Loss: 0.08831790834665298\n",
      "Child: 1, Epoch 17, Loss: 0.08057603985071182\n",
      "Child: 1, Epoch 18, Loss: 0.07367617636919022\n",
      "Child: 1, Epoch 19, Loss: 0.07493344694375992\n",
      "Child: 1, Epoch 20, Loss: 0.06179164722561836\n",
      "Child: 1, Epoch 21, Loss: 0.06480520963668823\n",
      "Child: 1, Epoch 22, Loss: 0.05591863393783569\n",
      "Child: 1, Epoch 23, Loss: 0.0599590539932251\n",
      "Child: 1, Epoch 24, Loss: 0.04940188303589821\n",
      "Child: 1, Epoch 25, Loss: 0.05602928251028061\n",
      "Child: 1, Epoch 26, Loss: 0.04913785308599472\n",
      "Child: 1, Epoch 27, Loss: 0.0460909903049469\n",
      "Child: 1, Epoch 28, Loss: 0.04939138889312744\n",
      "Child: 1, Epoch 29, Loss: 0.04341200739145279\n",
      "Child: 1, Epoch 30, Loss: 0.04646078497171402\n",
      "Child: 1, Epoch 31, Loss: 0.043704304844141006\n",
      "Child: 1, Epoch 32, Loss: 0.04278146103024483\n",
      "Child: 1, Epoch 33, Loss: 0.038973283022642136\n",
      "Child: 1, Epoch 34, Loss: 0.0423443503677845\n",
      "Child: 1, Epoch 35, Loss: 0.03750104457139969\n",
      "Child: 1, Epoch 36, Loss: 0.03933773189783096\n",
      "Child: 1, Epoch 37, Loss: 0.03477136418223381\n",
      "Child: 1, Epoch 38, Loss: 0.037162747234106064\n",
      "Child: 1, Epoch 39, Loss: 0.03463764861226082\n",
      "Child: 1, Epoch 40, Loss: 0.03503115847706795\n",
      "Testing Stats for Epoch 40\n",
      "    Mean Loss: 0.6543253660202026\n",
      "    Accuracy: 0.6392457867512098\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1917|FNs: 1139|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1023|TNs: 1914|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6520408163265307\n",
      "    Recall: 0.6272905759162304\n",
      "    F1 Score: 0.6394262841894598\n",
      "Training child #2\n",
      "Child: 2, Epoch 1, Loss: 0.7027385830879211\n",
      "Child: 2, Epoch 2, Loss: 0.6392670273780823\n",
      "Child: 2, Epoch 3, Loss: 0.5949767231941223\n",
      "Child: 2, Epoch 4, Loss: 0.5547086596488953\n",
      "Child: 2, Epoch 5, Loss: 0.49463799595832825\n",
      "Child: 2, Epoch 6, Loss: 0.432476669549942\n",
      "Child: 2, Epoch 7, Loss: 0.35711634159088135\n",
      "Child: 2, Epoch 8, Loss: 0.3027876019477844\n",
      "Child: 2, Epoch 9, Loss: 0.2471289187669754\n",
      "Child: 2, Epoch 10, Loss: 0.20194055140018463\n",
      "Child: 2, Epoch 11, Loss: 0.17314176261425018\n",
      "Child: 2, Epoch 12, Loss: 0.1454087346792221\n",
      "Child: 2, Epoch 13, Loss: 0.12427742034196854\n",
      "Child: 2, Epoch 14, Loss: 0.10832692682743073\n",
      "Child: 2, Epoch 15, Loss: 0.09493415802717209\n",
      "Child: 2, Epoch 16, Loss: 0.086493581533432\n",
      "Child: 2, Epoch 17, Loss: 0.07911849021911621\n",
      "Child: 2, Epoch 18, Loss: 0.07142292708158493\n",
      "Child: 2, Epoch 19, Loss: 0.06672201305627823\n",
      "Child: 2, Epoch 20, Loss: 0.06630658358335495\n",
      "Child: 2, Epoch 21, Loss: 0.0525338277220726\n",
      "Child: 2, Epoch 22, Loss: 0.055310167372226715\n",
      "Child: 2, Epoch 23, Loss: 0.05305130407214165\n",
      "Child: 2, Epoch 24, Loss: 0.05290483310818672\n",
      "Child: 2, Epoch 25, Loss: 0.04733928665518761\n",
      "Child: 2, Epoch 26, Loss: 0.04856487736105919\n",
      "Child: 2, Epoch 27, Loss: 0.04510781541466713\n",
      "Child: 2, Epoch 28, Loss: 0.04589524492621422\n",
      "Child: 2, Epoch 29, Loss: 0.046841979026794434\n",
      "Child: 2, Epoch 30, Loss: 0.04716010019183159\n",
      "Child: 2, Epoch 31, Loss: 0.03872738778591156\n",
      "Child: 2, Epoch 32, Loss: 0.038074642419815063\n",
      "Child: 2, Epoch 33, Loss: 0.03601550683379173\n",
      "Child: 2, Epoch 34, Loss: 0.0413176529109478\n",
      "Child: 2, Epoch 35, Loss: 0.03724760562181473\n",
      "Child: 2, Epoch 36, Loss: 0.036605291068553925\n",
      "Child: 2, Epoch 37, Loss: 0.03516123443841934\n",
      "Child: 2, Epoch 38, Loss: 0.036872781813144684\n",
      "Child: 2, Epoch 39, Loss: 0.033056698739528656\n",
      "Child: 2, Epoch 40, Loss: 0.033791910856962204\n",
      "Testing Stats for Epoch 40\n",
      "    Mean Loss: 0.7380396723747253\n",
      "    Accuracy: 0.6596028700150175\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 2014|FNs: 1042|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 998|TNs: 1939|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6686586985391766\n",
      "    Recall: 0.6590314136125655\n",
      "    F1 Score: 0.6638101516150295\n",
      "Training child #3\n",
      "Child: 3, Epoch 1, Loss: 0.6992530822753906\n",
      "Child: 3, Epoch 2, Loss: 0.6427707672119141\n",
      "Child: 3, Epoch 3, Loss: 0.5956358313560486\n",
      "Child: 3, Epoch 4, Loss: 0.549432635307312\n",
      "Child: 3, Epoch 5, Loss: 0.4880467355251312\n",
      "Child: 3, Epoch 6, Loss: 0.42271873354911804\n",
      "Child: 3, Epoch 7, Loss: 0.3551981449127197\n",
      "Child: 3, Epoch 8, Loss: 0.28801819682121277\n",
      "Child: 3, Epoch 9, Loss: 0.23720380663871765\n",
      "Child: 3, Epoch 10, Loss: 0.19276170432567596\n",
      "Child: 3, Epoch 11, Loss: 0.16541096568107605\n",
      "Child: 3, Epoch 12, Loss: 0.13524030148983002\n",
      "Child: 3, Epoch 13, Loss: 0.11562402546405792\n",
      "Child: 3, Epoch 14, Loss: 0.10759223252534866\n",
      "Child: 3, Epoch 15, Loss: 0.09418848156929016\n",
      "Child: 3, Epoch 16, Loss: 0.07785773277282715\n",
      "Child: 3, Epoch 17, Loss: 0.07657282054424286\n",
      "Child: 3, Epoch 18, Loss: 0.07191287726163864\n",
      "Child: 3, Epoch 19, Loss: 0.07440654933452606\n",
      "Child: 3, Epoch 20, Loss: 0.060311704874038696\n",
      "Child: 3, Epoch 21, Loss: 0.05876002088189125\n",
      "Child: 3, Epoch 22, Loss: 0.05973212420940399\n",
      "Child: 3, Epoch 23, Loss: 0.05422019958496094\n",
      "Child: 3, Epoch 24, Loss: 0.05501045286655426\n",
      "Child: 3, Epoch 25, Loss: 0.04565902054309845\n",
      "Child: 3, Epoch 26, Loss: 0.049589868634939194\n",
      "Child: 3, Epoch 27, Loss: 0.04728863388299942\n",
      "Child: 3, Epoch 28, Loss: 0.04322841018438339\n",
      "Child: 3, Epoch 29, Loss: 0.045279454439878464\n",
      "Child: 3, Epoch 30, Loss: 0.04541837424039841\n",
      "Child: 3, Epoch 31, Loss: 0.039948102086782455\n",
      "Child: 3, Epoch 32, Loss: 0.040484409779310226\n",
      "Child: 3, Epoch 33, Loss: 0.04384962096810341\n",
      "Child: 3, Epoch 34, Loss: 0.04039594158530235\n",
      "Child: 3, Epoch 35, Loss: 0.03990820422768593\n",
      "Child: 3, Epoch 36, Loss: 0.03424648195505142\n",
      "Child: 3, Epoch 37, Loss: 0.0367332398891449\n",
      "Child: 3, Epoch 38, Loss: 0.034258365631103516\n",
      "Child: 3, Epoch 39, Loss: 0.032764144241809845\n",
      "Child: 3, Epoch 40, Loss: 0.03373716026544571\n",
      "Testing Stats for Epoch 40\n",
      "    Mean Loss: 0.8810191750526428\n",
      "    Accuracy: 0.6666110462205906\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 2052|FNs: 1004|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 994|TNs: 1943|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6736703873933026\n",
      "    Recall: 0.6714659685863874\n",
      "    F1 Score: 0.6725663716814159\n",
      "Training child #4\n",
      "Child: 4, Epoch 1, Loss: 0.7037423849105835\n",
      "Child: 4, Epoch 2, Loss: 0.6451181769371033\n",
      "Child: 4, Epoch 3, Loss: 0.5945358872413635\n",
      "Child: 4, Epoch 4, Loss: 0.545564591884613\n",
      "Child: 4, Epoch 5, Loss: 0.4888382852077484\n",
      "Child: 4, Epoch 6, Loss: 0.421115905046463\n",
      "Child: 4, Epoch 7, Loss: 0.35817357897758484\n",
      "Child: 4, Epoch 8, Loss: 0.2899547219276428\n",
      "Child: 4, Epoch 9, Loss: 0.23964069783687592\n",
      "Child: 4, Epoch 10, Loss: 0.19266436994075775\n",
      "Child: 4, Epoch 11, Loss: 0.1683386266231537\n",
      "Child: 4, Epoch 12, Loss: 0.1423579901456833\n",
      "Child: 4, Epoch 13, Loss: 0.11883766204118729\n",
      "Child: 4, Epoch 14, Loss: 0.11014287918806076\n",
      "Child: 4, Epoch 15, Loss: 0.095675989985466\n",
      "Child: 4, Epoch 16, Loss: 0.08803284913301468\n",
      "Child: 4, Epoch 17, Loss: 0.07727710902690887\n",
      "Child: 4, Epoch 18, Loss: 0.07362347841262817\n",
      "Child: 4, Epoch 19, Loss: 0.06911533325910568\n",
      "Child: 4, Epoch 20, Loss: 0.059018395841121674\n",
      "Child: 4, Epoch 21, Loss: 0.056348320096731186\n",
      "Child: 4, Epoch 22, Loss: 0.05329512432217598\n",
      "Child: 4, Epoch 23, Loss: 0.05543563887476921\n",
      "Child: 4, Epoch 24, Loss: 0.05172020196914673\n",
      "Child: 4, Epoch 25, Loss: 0.054306406527757645\n",
      "Child: 4, Epoch 26, Loss: 0.050514623522758484\n",
      "Child: 4, Epoch 27, Loss: 0.04983038827776909\n",
      "Child: 4, Epoch 28, Loss: 0.04395453631877899\n",
      "Child: 4, Epoch 29, Loss: 0.04496781527996063\n",
      "Child: 4, Epoch 30, Loss: 0.041823454201221466\n",
      "Child: 4, Epoch 31, Loss: 0.040146540850400925\n",
      "Child: 4, Epoch 32, Loss: 0.04312128201127052\n",
      "Child: 4, Epoch 33, Loss: 0.038881566375494\n",
      "Child: 4, Epoch 34, Loss: 0.038839053362607956\n",
      "Child: 4, Epoch 35, Loss: 0.03947024419903755\n",
      "Child: 4, Epoch 36, Loss: 0.037342365831136703\n",
      "Child: 4, Epoch 37, Loss: 0.038621410727500916\n",
      "Child: 4, Epoch 38, Loss: 0.039984628558158875\n",
      "Child: 4, Epoch 39, Loss: 0.0343271903693676\n",
      "Child: 4, Epoch 40, Loss: 0.032243311405181885\n",
      "Testing Stats for Epoch 40\n",
      "    Mean Loss: 1.0266550779342651\n",
      "    Accuracy: 0.6726180543967962\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 2002|FNs: 1054|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 908|TNs: 2029|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6879725085910653\n",
      "    Recall: 0.6551047120418848\n",
      "    F1 Score: 0.6711364398256788\n",
      "Training child #5\n",
      "Child: 5, Epoch 1, Loss: 0.6959361433982849\n",
      "Child: 5, Epoch 2, Loss: 0.6374973058700562\n",
      "Child: 5, Epoch 3, Loss: 0.5913411974906921\n",
      "Child: 5, Epoch 4, Loss: 0.5397976636886597\n",
      "Child: 5, Epoch 5, Loss: 0.47624436020851135\n",
      "Child: 5, Epoch 6, Loss: 0.4082961678504944\n",
      "Child: 5, Epoch 7, Loss: 0.34088611602783203\n",
      "Child: 5, Epoch 8, Loss: 0.2775140106678009\n",
      "Child: 5, Epoch 9, Loss: 0.23313875496387482\n",
      "Child: 5, Epoch 10, Loss: 0.18535509705543518\n",
      "Child: 5, Epoch 11, Loss: 0.1580817997455597\n",
      "Child: 5, Epoch 12, Loss: 0.1396757960319519\n",
      "Child: 5, Epoch 13, Loss: 0.12292420864105225\n",
      "Child: 5, Epoch 14, Loss: 0.1035870909690857\n",
      "Child: 5, Epoch 15, Loss: 0.09461653977632523\n",
      "Child: 5, Epoch 16, Loss: 0.08331204205751419\n",
      "Child: 5, Epoch 17, Loss: 0.08006037026643753\n",
      "Child: 5, Epoch 18, Loss: 0.07128541171550751\n",
      "Child: 5, Epoch 19, Loss: 0.06457261741161346\n",
      "Child: 5, Epoch 20, Loss: 0.06591514497995377\n",
      "Child: 5, Epoch 21, Loss: 0.0589524582028389\n",
      "Child: 5, Epoch 22, Loss: 0.05667925626039505\n",
      "Child: 5, Epoch 23, Loss: 0.047403279691934586\n",
      "Child: 5, Epoch 24, Loss: 0.05751766264438629\n",
      "Child: 5, Epoch 25, Loss: 0.04495324566960335\n",
      "Child: 5, Epoch 26, Loss: 0.04984447732567787\n",
      "Child: 5, Epoch 27, Loss: 0.05057327076792717\n",
      "Child: 5, Epoch 28, Loss: 0.047012653201818466\n",
      "Child: 5, Epoch 29, Loss: 0.044140689074993134\n",
      "Child: 5, Epoch 30, Loss: 0.0433155819773674\n",
      "Child: 5, Epoch 31, Loss: 0.03953291103243828\n",
      "Child: 5, Epoch 32, Loss: 0.03838389739394188\n",
      "Child: 5, Epoch 33, Loss: 0.03840178996324539\n",
      "Child: 5, Epoch 34, Loss: 0.04322464019060135\n",
      "Child: 5, Epoch 35, Loss: 0.0353206992149353\n",
      "Child: 5, Epoch 36, Loss: 0.037284187972545624\n",
      "Child: 5, Epoch 37, Loss: 0.0326165035367012\n",
      "Child: 5, Epoch 38, Loss: 0.03197363391518593\n",
      "Child: 5, Epoch 39, Loss: 0.03162102773785591\n",
      "Child: 5, Epoch 40, Loss: 0.035801373422145844\n",
      "Testing Stats for Epoch 40\n",
      "    Mean Loss: 1.2083250284194946\n",
      "    Accuracy: 0.6704488569998331\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 2062|FNs: 994|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 981|TNs: 1956|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6776207689779823\n",
      "    Recall: 0.6747382198952879\n",
      "    F1 Score: 0.6761764223643221\n"
     ]
    }
   ],
   "source": [
    "if train_new_classifier:\n",
    "    mpnet_classifier = train_classifier()\n",
    "    torch.save(mpnet_classifier.state_dict(), \"Cached_MPNet/trained_classifier_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 5993 MPNet outputs\n",
      "Testing Stats for MPNet Classifer (Threshold: 0)\n",
      "    Mean Loss: 1.2083250284194946\n",
      "    Accuracy: 0.6704488569998331\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 2062|FNs: 994|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 981|TNs: 1956|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6776207689779823\n",
      "    Recall: 0.6747382198952879\n",
      "    F1 Score: 0.6761764223643221\n"
     ]
    }
   ],
   "source": [
    "if evaluate_classifier:\n",
    "    mpnet_classifier = AverageClassifier(children=5).to(device)\n",
    "    mpnet_classifier.load_state_dict(torch.load(\"Cached_MPNet/trained_classifier_weights.pt\", weights_only=True, map_location=device))\n",
    "    eval_classifier(mpnet_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 5985 MPNet outputs\n"
     ]
    }
   ],
   "source": [
    "if test_the_classifier:\n",
    "    mpnet_classifier = AverageClassifier().to(device)\n",
    "    mpnet_classifier.load_state_dict(torch.load(\"Cached_MPNet/trained_classifier_weights.pt\", weights_only=True, map_location=device))\n",
    "    predictions = test_classifier(mpnet_classifier, test_data_path)\n",
    "    preds_df = pd.DataFrame(predictions)\n",
    "    preds_df.to_csv(\"Group_17_B.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 5993 MPNet outputs\n",
      "Done Test\n"
     ]
    }
   ],
   "source": [
    "if generate_evaluation_csv:\n",
    "    mpnet_classifier = AverageClassifier().to(device)\n",
    "    mpnet_classifier.load_state_dict(torch.load(\"Cached_MPNet/trained_classifier_weights.pt\", weights_only=True, map_location=device))\n",
    "    predictions = test_classifier(mpnet_classifier, \"Data/dev.csv\")\n",
    "    print(\"Done Test\")\n",
    "    predictions = [\"prediction\"] + predictions\n",
    "    preds_df = pd.DataFrame(predictions)\n",
    "    preds_df.to_csv(\"MPNet_Eval.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained weights\n",
    "mpnet_classifier = AverageClassifier().to(device)\n",
    "mpnet_classifier.load_state_dict(torch.load(\"Cached_MPNet/trained_classifier_weights.pt\", weights_only=True, map_location=device))\n",
    "\n",
    "demo_path = \"demo.csv\"\n",
    "predictions = test_classifier(mpnet_classifier, demo_path)\n",
    "print(\"Created Demo Predictions\")\n",
    "preds_df = pd.DataFrame(predictions)\n",
    "preds_df.to_csv(\"Demo_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
