{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplearning with Transformer Architectures\n",
    "\n",
    "(Explanation of method here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\boltm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import all relevant modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BigBirdModel, BigBirdTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessing\n",
    "\n",
    "def preprocess_line(line: str, params: set, tokenizer=None) -> list[str]:\n",
    "    '''\n",
    "    Preprocesses a line of text\n",
    "\n",
    "    :param line:\n",
    "    :param params:\n",
    "    :return:\n",
    "\n",
    "    Program flow:\n",
    "        If the line contains an email, trims out the email header\n",
    "        Tokenises the line\n",
    "        Applies various transformations\n",
    "            Removes stop words\n",
    "            Stems\n",
    "            Lemmatises\n",
    "            Remvoes non-alphanumeric characters\n",
    "            Sets all lowercase\n",
    "        Returns the list of tokens\n",
    "    '''\n",
    "\n",
    "    if \"trim email\" in params:\n",
    "        if \"-- Forwarded by\" in line:\n",
    "            before_email = line.split(\"-\")[0].strip()\n",
    "            email = \"-\" + line.split(\"-\", 1)[1] if \"-\" in line else \"\"\n",
    "            email_subject = email.split(\"Subject:\")[-1].strip() # if there's no subject, this keeps the whole email\n",
    "            line = before_email + \" \" + email_subject\n",
    "            line = line.strip()\n",
    "        line\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        tokens = nltk.tokenize.word_tokenize(line)\n",
    "    else:\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        starts_with_space_labels = [1 if \"Ġ\" in token else 0 for token in tokens]\n",
    "        tokens = [token.replace(\"Ġ\",\"\") if \"Ġ\" in token else token for token in tokens ]\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "\n",
    "    operations = {\n",
    "        \"stop words\": lambda tokens: [token if token.lower() not in nltk.corpus.stopwords.words('english') else \"\" for token in tokens],\n",
    "        \"stem\": lambda tokens: [nltk.PorterStemmer().stem(token) for token in tokens],\n",
    "        \"lemmatise\": lambda tokens: [nltk.WordNetLemmatizer().lemmatize(token) for token in tokens],\n",
    "        \"alphanumeric\": lambda tokens: [\"\".join(filter(str.isalnum, token)) for token in tokens],\n",
    "        \"lowercase\": lambda tokens: [token.lower() for token in tokens]\n",
    "    }\n",
    "\n",
    "    # Apply each operation if we define it in the params set\n",
    "    for key, action in operations.items():\n",
    "        if key in params:\n",
    "            tokens = action(tokens)\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        return [token for token in tokens if token != \"\"]\n",
    "    else:\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "        return [tokens[i] if not starts_with_space_labels[i] else \"\".join([\"Ġ\", tokens[i]]) for i in range(len(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     halloween with kids is fun - getting them to b...\n",
      "1     everybody sing! urlLink http://www.smokeybear....\n",
      "2     'The preservation of health is a duty. Few see...\n",
      "3     Following 1940's SAPS AT SEA , Stan Laurel and...\n",
      "4     Marybeth: The call's ended and I'll be availab...\n",
      "                            ...                        \n",
      "95    I have created a new subdirectory aptly called...\n",
      "96    Well there isnt much to wirte about god damn t...\n",
      "97    urlLink Unstuff this file in Applications/Ghos...\n",
      "98    urlLink FOXNews.com - Top Stories - World Join...\n",
      "99    ACCESS.BENEFITS.2001 Benefits Customer Service...\n",
      "Length: 100, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention type 'block_sparse' is not possible if sequence_length: 514 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    }
   ],
   "source": [
    "# Define and create dataset and data loader\n",
    "\n",
    "class reformattedDataset(Dataset):\n",
    "    def __init__(self, df, parameters, reset_cache=False):\n",
    "        \n",
    "        if os.path.exists(\"Cached_Roberta/output1_roberta.pt\") and os.path.exists(\"Cached_Roberta/output2_roberta.pt\") and reset_cache == False:\n",
    "            print(\"Loading cached Roberta outputs...\")\n",
    "            self.output1_roberta = torch.stack(torch.load(\"Cached_Roberta/output1_roberta.pt\", weights_only=True))\n",
    "            self.output2_roberta = torch.stack(torch.load(\"Cached_Roberta/output2_roberta.pt\", weights_only=True))\n",
    "            print(f\"Successfully loaded {len(self.output1_roberta)} Roberta outputs\")\n",
    "\n",
    "            self.labels = df[\"label\"][:len(self.output1_roberta)].reset_index(drop=True)\n",
    "\n",
    "        else:\n",
    "            tokenizer = BigBirdTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "            texts1 = pd.Series([tokenizer.convert_tokens_to_string(preprocess_line(row,parameters,tokenizer)) for row in df[\"text_1\"]])\n",
    "            texts2 = pd.Series([tokenizer.convert_tokens_to_string(preprocess_line(row,parameters,tokenizer)) for row in df[\"text_2\"]])\n",
    "            print(texts1)\n",
    "\n",
    "            encodeds1 = pd.Series([tokenizer(text, return_tensors = \"pt\", padding=\"max_length\", max_length=514, truncation=True) for text in texts1])\n",
    "            encodeds2 = pd.Series([tokenizer(text, return_tensors = \"pt\", padding=\"max_length\", max_length=514, truncation=True) for text in texts2])\n",
    "\n",
    "            self.inputs1 = pd.Series([encoded[\"input_ids\"].squeeze(0) for encoded in encodeds1])\n",
    "            self.attention_masks1 = pd.Series([encoded[\"attention_mask\"].squeeze(0) for encoded in encodeds1])\n",
    "            self.inputs2 = pd.Series([encoded[\"input_ids\"].squeeze(0) for encoded in encodeds2])\n",
    "            self.attention_masks2 = pd.Series([encoded[\"attention_mask\"].squeeze(0) for encoded in encodeds2])\n",
    "\n",
    "            # pre compute roberta tensors to save time\n",
    "            self.roberta = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\").to(device)\n",
    "            for parameter in self.roberta.parameters():\n",
    "                parameter.requires_grad = False\n",
    "            self.output1_roberta = torch.stack([self.roberta(input1.unsqueeze(0).to(device), attention_mask=attention_mask1.unsqueeze(0).to(device)).last_hidden_state[:, 0, :].squeeze(0).detach() for input1, attention_mask1 in zip(self.inputs1, self.attention_masks1)])\n",
    "            self.output2_roberta = torch.stack([self.roberta(input2.unsqueeze(0).to(device), attention_mask=attention_mask2.unsqueeze(0).to(device)).last_hidden_state[:, 0, :].squeeze(0).detach() for input2, attention_mask2 in zip(self.inputs2, self.attention_masks2)])\n",
    "\n",
    "            # save roberta tensors to file, prevents rerunning each time\n",
    "            torch.save(self.output1_roberta.tolist(), \"Cached_Roberta/output1_roberta.pt\")\n",
    "            torch.save(self.output2_roberta.tolist(), \"Cached_Roberta/output2_roberta.pt\")\n",
    "\n",
    "\n",
    "            self.labels = df[\"label\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.output1_roberta)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        return {\n",
    "            \"input1\": self.output1_roberta[i],\n",
    "            \"input2\": self.output2_roberta[i],\n",
    "            \"label\": torch.tensor(self.labels.iloc[i], dtype=torch.float).to(device)\n",
    "        }\n",
    "\n",
    "dataset = reformattedDataset(pd.read_csv(\"Data/train.csv\", nrows=27644), parameters={}, reset_cache=True) #TODO try parameters\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our architecture\n",
    "\n",
    "class classifierRoberta(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classifierRoberta, self).__init__()\n",
    "\n",
    "        # load and freeze pretrained Roberta\n",
    "        self.roberta = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "        for parameter in self.roberta.parameters():\n",
    "            parameter.requires_grad = False \n",
    "        \n",
    "        # add our custom binary classifier layer to end\n",
    "        self.classifier = nn.Linear(768*2, 1)\n",
    "        self.sigmoid = nn.Sigmoid() #to convert classifier output to probability\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        \n",
    "        # use pre computed Roberta tensors\n",
    "        combined_inputs = torch.cat((input1.squeeze(1), input2.squeeze(1)), dim=1)\n",
    "\n",
    "        # pass result through our appended classifier layers\n",
    "        classifier_output = self.classifier(combined_inputs)\n",
    "        probability = self.sigmoid(classifier_output)\n",
    "        return probability.squeeze() #convert to 1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.7076087594032288  Epoch: 0 \n",
      "Loss 0.6445196270942688  Epoch: 1000 \n",
      "Loss 0.6001457571983337  Epoch: 2000 \n",
      "Loss 0.5600441694259644  Epoch: 3000 \n",
      "Loss 0.523388147354126  Epoch: 4000 \n",
      "Loss 0.4896755814552307  Epoch: 5000 \n",
      "Loss 0.4585280120372772  Epoch: 6000 \n",
      "Loss 0.4296446144580841  Epoch: 7000 \n",
      "Loss 0.4027692973613739  Epoch: 8000 \n",
      "Loss 0.3776927888393402  Epoch: 9000 \n",
      "Loss 0.35425063967704773  Epoch: 10000 \n",
      "Loss 0.33230605721473694  Epoch: 11000 \n",
      "Loss 0.3117361068725586  Epoch: 12000 \n",
      "Loss 0.2924351692199707  Epoch: 13000 \n",
      "Loss 0.27430781722068787  Epoch: 14000 \n",
      "Loss 0.25726643204689026  Epoch: 15000 \n",
      "Loss 0.2412378191947937  Epoch: 16000 \n",
      "Loss 0.2261602133512497  Epoch: 17000 \n",
      "Loss 0.21198096871376038  Epoch: 18000 \n",
      "Loss 0.1986524909734726  Epoch: 19000 \n",
      "Loss 0.18612729012966156  Epoch: 20000 \n",
      "Loss 0.17435844242572784  Epoch: 21000 \n",
      "Loss 0.1633017659187317  Epoch: 22000 \n",
      "Loss 0.15291500091552734  Epoch: 23000 \n",
      "Loss 0.1431588977575302  Epoch: 24000 \n"
     ]
    }
   ],
   "source": [
    "# Create and train model\n",
    "\n",
    "model = classifierRoberta().to(device)\n",
    "optimiser = torch.optim.AdamW(model.classifier.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "loss_model = nn.BCELoss()\n",
    "\n",
    "epochs = 25_000\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        input_1 = batch[\"input1\"].to(device)\n",
    "        input_2 = batch[\"input2\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        predictions = model(input_1, input_2)\n",
    "        \n",
    "        loss = loss_model(predictions, label)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Loss {total_loss/len(dataloader)}  Epoch: {epoch} \") \n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"Cached_Roberta/trained_classifier_weights.pt\")\n",
    "\n",
    "# load model\n",
    "# model = classifierRoberta().to(device)\n",
    "# model.load_state_dict(torch.load(\"Cached_Roberta/trained_classifier_weights.pt\", weights_only=True))\n",
    "# model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO find optimal threshhold after training\n",
    "\n",
    "T = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def makePrediction(index):\n",
    "    data = dataset[index]\n",
    "    input_1 = data[\"input1\"].to(device)\n",
    "    input_2 = data[\"input2\"].to(device)\n",
    "    label = data[\"label\"].to(device)\n",
    "\n",
    "    prediction = model(input_1.unsqueeze(0), input_2.unsqueeze(0))\n",
    "    return prediction.item(), label.item()\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(dataset)):\n",
    "    prediction, label = makePrediction(i)\n",
    "    prediction = 1 if prediction > T else 0 \n",
    "    predictions.append((prediction,int(label)))\n",
    "    print(predictions[i])\n",
    "\n",
    "correct = sum(pred == label for pred, label in predictions)\n",
    "accuracy = correct / len(predictions)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
