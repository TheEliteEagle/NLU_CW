{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplearning with Transformer Architectures\n",
    "\n",
    "(Explanation of method here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Imports and PIP Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /home/benjamin/.local/lib/python3.10/site-packages (4.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (0.30.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (4.50.3)\n",
      "Requirement already satisfied: scikit-learn in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: tqdm in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: scipy in /home/benjamin/.local/lib/python3.10/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/benjamin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: requests in /home/benjamin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/benjamin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/benjamin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/benjamin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: jinja2 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/benjamin/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/benjamin/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/benjamin/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/benjamin/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/benjamin/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/benjamin/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/benjamin/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/benjamin/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/benjamin/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/benjamin/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentence-transformers\n",
    "!pip3 inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x742cc81fb550>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all relevant modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class and Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessing\n",
    "\n",
    "def preprocess_line(line: str, params: set, tokenizer=None) -> list[str]:\n",
    "    '''\n",
    "    Preprocesses a line of text\n",
    "\n",
    "    :param line:\n",
    "    :param params:\n",
    "    :return:\n",
    "\n",
    "    Program flow:\n",
    "        If the line contains an email, trims out the email header\n",
    "        Tokenises the line\n",
    "        Applies various transformations\n",
    "            Removes stop words\n",
    "            Stems\n",
    "            Lemmatises\n",
    "            Remvoes non-alphanumeric characters\n",
    "            Sets all lowercase\n",
    "        Returns the list of tokens\n",
    "    '''\n",
    "\n",
    "    if \"trim email\" in params:\n",
    "        if \"-- Forwarded by\" in line:\n",
    "            before_email = line.split(\"-\")[0].strip()\n",
    "            email = \"-\" + line.split(\"-\", 1)[1] if \"-\" in line else \"\"\n",
    "            email_subject = email.split(\"Subject:\")[-1].strip() # if there's no subject, this keeps the whole email\n",
    "            line = before_email + \" \" + email_subject\n",
    "            line = line.strip()\n",
    "        line\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        tokens = nltk.tokenize.word_tokenize(line)\n",
    "    else:\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        starts_with_space_labels = [1 if \"Ġ\" in token else 0 for token in tokens]\n",
    "        tokens = [token.replace(\"Ġ\",\"\") if \"Ġ\" in token else token for token in tokens ]\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "\n",
    "    operations = {\n",
    "        \"stop words\": lambda tokens: [token if token.lower() not in nltk.corpus.stopwords.words('english') else \"\" for token in tokens],\n",
    "        \"stem\": lambda tokens: [nltk.PorterStemmer().stem(token) for token in tokens],\n",
    "        \"lemmatise\": lambda tokens: [nltk.WordNetLemmatizer().lemmatize(token) for token in tokens],\n",
    "        \"alphanumeric\": lambda tokens: [\"\".join(filter(str.isalnum, token)) for token in tokens],\n",
    "        \"lowercase\": lambda tokens: [token.lower() for token in tokens]\n",
    "    }\n",
    "\n",
    "    # Apply each operation if we define it in the params set\n",
    "    for key, action in operations.items():\n",
    "        if key in params:\n",
    "            tokens = action(tokens)\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        return [token for token in tokens if token != \"\"]\n",
    "    else:\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "        return [tokens[i] if not starts_with_space_labels[i] else \"\".join([\"Ġ\", tokens[i]]) for i in range(len(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and create dataset and data loader\n",
    "\n",
    "class reformattedDataset(Dataset):\n",
    "    def __init__(self, df_name, parameters, reset_cache=False):\n",
    "        # Name is in form <path>/<name>.csv\n",
    "        name = df_name.split(\"/\")[-1]\n",
    "        name = name.split(\".\")[0]\n",
    "        print(name)\n",
    "\n",
    "        df = pd.read_csv(df_name)\n",
    "        \n",
    "        if os.path.exists(f\"Cached_MPNet/{name}_output1.pt\") and os.path.exists(f\"Cached_MPNet/{name}_output2.pt\") and reset_cache == False:\n",
    "            print(\"Loading cached MPNet outputs...\")\n",
    "            loaded_o1 = torch.load(f\"Cached_MPNet/{name}_output1.pt\", weights_only=True)\n",
    "            self.output1 = torch.stack([torch.tensor(t) for t in loaded_o1])\n",
    "            self.output2 = torch.stack([torch.tensor(t) for t in torch.load(f\"Cached_MPNet/{name}_output2.pt\", weights_only=True)])\n",
    "            print(f\"Successfully loaded {len(self.output1)} MPNet outputs\")\n",
    "\n",
    "            self.labels = df[\"label\"][:len(self.output1)].reset_index(drop=True)\n",
    "\n",
    "        else:\n",
    "            mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "            texts1 = df[\"text_1\"].astype(str).tolist()\n",
    "            texts2 = df[\"text_2\"].astype(str).tolist()\n",
    "\n",
    "            self.output1 = torch.tensor(mpnet.encode(texts1, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            self.output2 = torch.tensor(mpnet.encode(texts2, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            \n",
    "            self.labels = df[\"label\"]\n",
    "\n",
    "            torch.save(self.output1, f\"Cached_MPNet/{name}_output1.pt\")\n",
    "            torch.save(self.output2, f\"Cached_MPNet/{name}_output2.pt\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.output1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        return self.output1[i], self.output2[i], torch.tensor(self.labels.iloc[i], dtype=torch.float)\n",
    "\n",
    "class MPNetTestDataset(Dataset):\n",
    "    def __init__(self, df_name, parameters, reset_cache=False):\n",
    "        # Name is in form <path>/<name>.csv\n",
    "        name = df_name.split(\"/\")[-1]\n",
    "        name = name.split(\".\")[0]\n",
    "        print(name)\n",
    "\n",
    "        df = pd.read_csv(df_name)\n",
    "        \n",
    "        if os.path.exists(f\"Cached_MPNet/{name}_output1.pt\") and os.path.exists(f\"Cached_MPNet/{name}_output2.pt\") and reset_cache == False:\n",
    "            print(\"Loading cached MPNet outputs...\")\n",
    "            loaded_o1 = torch.load(f\"Cached_MPNet/{name}_output1.pt\", weights_only=True)\n",
    "            self.output1 = torch.stack([torch.tensor(t) for t in loaded_o1])\n",
    "            self.output2 = torch.stack([torch.tensor(t) for t in torch.load(f\"Cached_MPNet/{name}_output2.pt\", weights_only=True)])\n",
    "            print(f\"Successfully loaded {len(self.output1)} MPNet outputs\")\n",
    "\n",
    "        else:\n",
    "            mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "            texts1 = df[\"text_1\"].astype(str).tolist()\n",
    "            texts2 = df[\"text_2\"].astype(str).tolist()\n",
    "\n",
    "            self.output1 = torch.tensor(mpnet.encode(texts1, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            self.output2 = torch.tensor(mpnet.encode(texts2, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            \n",
    "            torch.save(self.output1, f\"Cached_MPNet/{name}_output1.pt\")\n",
    "            torch.save(self.output2, f\"Cached_MPNet/{name}_output2.pt\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.output1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        return self.output1[i], self.output2[i]\n",
    "\n",
    "# dataset = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=False) #TODO try parameters\n",
    "# val_dataset = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=True)\n",
    "# test_dataset = reformattedDataset(\"Data/AV_trial.csv\", parameters={}, reset_cache=True)\n",
    "# dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our architecture\n",
    "\n",
    "class classifierMPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classifierMPNet, self).__init__()\n",
    "\n",
    "        # add our custom binary classifier layer to end\n",
    "        self.fc1 = nn.Linear(768*4 + 1, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            self.fc1,\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.Dropout(0.3),\n",
    "            self.fc2, \n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.3),\n",
    "            self.fc3\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid() #to convert classifier output to probability\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # use pre computed MPNet tensors\n",
    "        cos_sim = nn.functional.cosine_similarity(input1, input2, dim=1).unsqueeze(1)\n",
    "        combined_inputs = torch.cat((input1, input2, torch.abs(input1-input2), input1*input2, cos_sim), dim=1)\n",
    "\n",
    "        # pass result through our appended classifier layers\n",
    "        classifier_output = self.classifier(combined_inputs)\n",
    "        return classifier_output\n",
    "\n",
    "\n",
    "class AverageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AverageClassifier, self).__init__()\n",
    "        self.clfs = nn.ModuleList([classifierMPNet() for _ in range(5)])\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        outs = [clf(input1, input2) for clf in self.clfs]\n",
    "        stack = torch.stack(outs, dim=0)\n",
    "        return torch.mean(stack, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier():\n",
    "    # Hyper-Parameters:\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "    epochs = 20\n",
    "    threshold = 0\n",
    "    weight_decay = 0.01\n",
    "\n",
    "    model = AverageClassifier().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    train_ds = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=False)\n",
    "    val_ds = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=False)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_list = []\n",
    "\n",
    "        for s1, s2, l in train_dl:\n",
    "            s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            output = model(s1, s2).reshape(l.shape[0])\n",
    "            loss = criterion(output, l)\n",
    "            loss_list.append(loss.detach().cpu().numpy())\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        print(f\"Mean training loss for epoch {epoch+1}: {np.mean(loss_list)}\")\n",
    "\n",
    "        loss_list = []\n",
    "        preds = []\n",
    "        true_labels = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for s1, s2, l in val_dl:\n",
    "                s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "                output = model(s1, s2).reshape(l.shape[0])\n",
    "                loss = criterion(output, l)\n",
    "                loss_list.append(loss.detach().cpu().numpy())\n",
    "                preds.append(1 if output[0] > threshold else 0)\n",
    "                true_labels.append(l.detach().cpu().numpy())\n",
    "\n",
    "        correct = 0\n",
    "        TPs, FPs, FNs, TNs = 0, 0, 0, 0\n",
    "        for j in range(len(true_labels)):\n",
    "            if true_labels[j] == preds[j]:\n",
    "                correct += 1\n",
    "                if true_labels[j] == 1:\n",
    "                    TPs += 1\n",
    "                else:\n",
    "                    TNs += 1\n",
    "            else:\n",
    "                if true_labels[j] == 1:\n",
    "                    FNs += 1\n",
    "                else:\n",
    "                    FPs += 1\n",
    "\n",
    "        accuracy = correct / len(true_labels)\n",
    "        recall = 0 if TPs + FNs == 0 else TPs / (TPs + FNs)\n",
    "        precision = 0 if TPs + FPs == 0 else TPs / (TPs + FPs)\n",
    "        f1 = 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        print(f\"Testing Stats for Epoch {epoch+1}\")\n",
    "        print(f\"    Mean Loss: {np.mean(loss_list)}\")\n",
    "        print(f\"    Accuracy: {accuracy}\")\n",
    "        print(f\"    _____________________________\")\n",
    "        print(f\"    |True\\Pred|Positive|Negative|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    |Positive |TPs: {TPs}|FNs: {FNs}|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    |Negative |FPs: {FPs}|TNs: {TNs}|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    \")\n",
    "        print(f\"    Precision: {precision}\")\n",
    "        print(f\"    Recall: {recall}\")\n",
    "        print(f\"    F1 Score: {f1}\")\n",
    "\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(model):\n",
    "    val_ds = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=False)\n",
    "    val_dl = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    threshold = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for s1, s2, l in val_dl:\n",
    "            s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "            output = model(s1, s2).reshape(l.shape[0])\n",
    "            loss = criterion(output, l)\n",
    "            loss_list.append(loss.detach().cpu().numpy())\n",
    "            preds.append(1 if output[0] > threshold else 0)\n",
    "            true_labels.append(l.detach().cpu().numpy())\n",
    "\n",
    "    correct = 0\n",
    "    TPs, FPs, FNs, TNs = 0, 0, 0, 0\n",
    "    for j in range(len(true_labels)):\n",
    "        if true_labels[j] == preds[j]:\n",
    "            correct += 1\n",
    "            if true_labels[j] == 1:\n",
    "                TPs += 1\n",
    "            else:\n",
    "                TNs += 1\n",
    "        else:\n",
    "            if true_labels[j] == 1:\n",
    "                FNs += 1\n",
    "            else:\n",
    "                FPs += 1\n",
    "\n",
    "    accuracy = correct / len(true_labels)\n",
    "    recall = 0 if TPs + FNs == 0 else TPs / (TPs + FNs)\n",
    "    precision = 0 if TPs + FPs == 0 else TPs / (TPs + FPs)\n",
    "    f1 = 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Testing Stats for MPNet Classifer (Threshold: {threshold})\")\n",
    "    print(f\"    Mean Loss: {np.mean(loss_list)}\")\n",
    "    print(f\"    Accuracy: {accuracy}\")\n",
    "    print(f\"    _____________________________\")\n",
    "    print(f\"    |True\\Pred|Positive|Negative|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    |Positive |TPs: {TPs}|FNs: {FNs}|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    |Negative |FPs: {FPs}|TNs: {TNs}|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    \")\n",
    "    print(f\"    Precision: {precision}\")\n",
    "    print(f\"    Recall: {recall}\")\n",
    "    print(f\"    F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model, data_path):\n",
    "    test_ds = MPNetTestDataset(test_data_path, parameters={}, reset_cache=False)\n",
    "    test_dl = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    loss_list = []\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    threshold = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s1, s2 in test_dl:\n",
    "            s1, s2 = s1.to(device), s2.to(device)\n",
    "\n",
    "            output = model(s1, s2)\n",
    "            preds.append(1 if output[0] > threshold else 0)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook \"Engine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook parameters - decides which of the next cells actually run\n",
    "generate_new_encodings = False\n",
    "train_new_classifier = False\n",
    "test_the_classifier = True\n",
    "generate_evaluation_csv = True\n",
    "evaluate_classifier = True\n",
    "\n",
    "test_data_path = \"Data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset - run_next usually set to False as this means you can spam shift+enter to get through notebook\n",
    "if generate_new_encodings:\n",
    "    dataset = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=True) #TODO try parameters\n",
    "    val_dataset = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_new_classifier:\n",
    "    mpnet_classifier = train_classifier()\n",
    "    torch.save(mpnet_classifier.state_dict(), \"Cached_MPNet/trained_classifier_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 5993 MPNet outputs\n",
      "Testing Stats for MPNet Classifer (Threshold: 0)\n",
      "    Mean Loss: 1.5296602249145508\n",
      "    Accuracy: 0.6821291506757884\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 2023|FNs: 1033|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 872|TNs: 2065|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6987910189982729\n",
      "    Recall: 0.661976439790576\n",
      "    F1 Score: 0.6798857334901698\n"
     ]
    }
   ],
   "source": [
    "if evaluate_classifier:\n",
    "    mpnet_classifier = AverageClassifier().to(device)\n",
    "    mpnet_classifier.load_state_dict(torch.load(\"Cached_MPNet/trained_classifier_weights.pt\", weights_only=True))\n",
    "    eval_classifier(mpnet_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Loading cached MPNet outputs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52368/3139894658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.output1 = torch.stack([torch.tensor(t) for t in loaded_o1])\n",
      "/tmp/ipykernel_52368/3139894658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.output2 = torch.stack([torch.tensor(t) for t in torch.load(f\"Cached_MPNet/{name}_output2.pt\", weights_only=True)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 5985 MPNet outputs\n"
     ]
    }
   ],
   "source": [
    "if test_the_classifier:\n",
    "    mpnet_classifier = AverageClassifier().to(device)\n",
    "    mpnet_classifier.load_state_dict(torch.load(\"Cached_MPNet/trained_classifier_weights.pt\", weights_only=True))\n",
    "    predictions = test_classifier(mpnet_classifier, test_data_path)\n",
    "    preds_df = pd.DataFrame(predictions)\n",
    "    preds_df.to_csv(\"Group_17_C.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 5985 MPNet outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52368/3139894658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.output1 = torch.stack([torch.tensor(t) for t in loaded_o1])\n",
      "/tmp/ipykernel_52368/3139894658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.output2 = torch.stack([torch.tensor(t) for t in torch.load(f\"Cached_MPNet/{name}_output2.pt\", weights_only=True)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Test\n"
     ]
    }
   ],
   "source": [
    "if generate_evaluation_csv:\n",
    "    mpnet_classifier = AverageClassifier().to(device)\n",
    "    mpnet_classifier.load_state_dict(torch.load(\"Cached_MPNet/trained_classifier_weights.pt\", weights_only=True))\n",
    "    predictions = test_classifier(mpnet_classifier, \"Data/dev.csv\")\n",
    "    print(\"Done Test\")\n",
    "    predictions = [\"prediction\"] + predictions\n",
    "    preds_df = pd.DataFrame(predictions)\n",
    "    preds_df.to_csv(\"MPNet_Eval.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
