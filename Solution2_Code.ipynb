{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplearning with Transformer Architectures\n",
    "\n",
    "(Explanation of method here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Imports and PIP Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.48.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.6.0+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.26.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\boltm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: unknown command \"inst\" - maybe you meant \"list\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentence-transformers\n",
    "!pip3 inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22fcd005970>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all relevant modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class and Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessing\n",
    "\n",
    "def preprocess_line(line: str, params: set, tokenizer=None) -> list[str]:\n",
    "    '''\n",
    "    Preprocesses a line of text\n",
    "\n",
    "    :param line:\n",
    "    :param params:\n",
    "    :return:\n",
    "\n",
    "    Program flow:\n",
    "        If the line contains an email, trims out the email header\n",
    "        Tokenises the line\n",
    "        Applies various transformations\n",
    "            Removes stop words\n",
    "            Stems\n",
    "            Lemmatises\n",
    "            Remvoes non-alphanumeric characters\n",
    "            Sets all lowercase\n",
    "        Returns the list of tokens\n",
    "    '''\n",
    "\n",
    "    if \"trim email\" in params:\n",
    "        if \"-- Forwarded by\" in line:\n",
    "            before_email = line.split(\"-\")[0].strip()\n",
    "            email = \"-\" + line.split(\"-\", 1)[1] if \"-\" in line else \"\"\n",
    "            email_subject = email.split(\"Subject:\")[-1].strip() # if there's no subject, this keeps the whole email\n",
    "            line = before_email + \" \" + email_subject\n",
    "            line = line.strip()\n",
    "        line\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        tokens = nltk.tokenize.word_tokenize(line)\n",
    "    else:\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        starts_with_space_labels = [1 if \"Ġ\" in token else 0 for token in tokens]\n",
    "        tokens = [token.replace(\"Ġ\",\"\") if \"Ġ\" in token else token for token in tokens ]\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "\n",
    "    operations = {\n",
    "        \"stop words\": lambda tokens: [token if token.lower() not in nltk.corpus.stopwords.words('english') else \"\" for token in tokens],\n",
    "        \"stem\": lambda tokens: [nltk.PorterStemmer().stem(token) for token in tokens],\n",
    "        \"lemmatise\": lambda tokens: [nltk.WordNetLemmatizer().lemmatize(token) for token in tokens],\n",
    "        \"alphanumeric\": lambda tokens: [\"\".join(filter(str.isalnum, token)) for token in tokens],\n",
    "        \"lowercase\": lambda tokens: [token.lower() for token in tokens]\n",
    "    }\n",
    "\n",
    "    # Apply each operation if we define it in the params set\n",
    "    for key, action in operations.items():\n",
    "        if key in params:\n",
    "            tokens = action(tokens)\n",
    "    \n",
    "    if tokenizer == None:\n",
    "        return [token for token in tokens if token != \"\"]\n",
    "    else:\n",
    "        if len(tokens) != len(starts_with_space_labels):\n",
    "            print(len(tokens))\n",
    "            print(len(starts_with_space_labels))\n",
    "            raise Exception(\"A token has been removed, labels dont align\")\n",
    "        return [tokens[i] if not starts_with_space_labels[i] else \"\".join([\"Ġ\", tokens[i]]) for i in range(len(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and create dataset and data loader\n",
    "\n",
    "class reformattedDataset(Dataset):\n",
    "    def __init__(self, df_name, parameters, reset_cache=False):\n",
    "        # Name is in form <path>/<name>.csv\n",
    "        name = df_name.split(\"/\")[-1]\n",
    "        name = name.split(\".\")[0]\n",
    "        print(name)\n",
    "\n",
    "        df = pd.read_csv(df_name)\n",
    "        \n",
    "        if os.path.exists(f\"Cached_MPNet/{name}_output1.pt\") and os.path.exists(f\"Cached_MPNet/{name}_output2.pt\") and reset_cache == False:\n",
    "            print(\"Loading cached MPNet outputs...\")\n",
    "            loaded_o1 = torch.load(f\"Cached_MPNet/{name}_output1.pt\", weights_only=True)\n",
    "            self.output1 = torch.stack([torch.tensor(t) for t in loaded_o1])\n",
    "            self.output2 = torch.stack([torch.tensor(t) for t in torch.load(f\"Cached_MPNet/{name}_output2.pt\", weights_only=True)])\n",
    "            print(f\"Successfully loaded {len(self.output1)} MPNet outputs\")\n",
    "\n",
    "            self.labels = df[\"label\"][:len(self.output1)].reset_index(drop=True)\n",
    "\n",
    "        else:\n",
    "            mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "            texts1 = df[\"text_1\"].astype(str).tolist()\n",
    "            texts2 = df[\"text_2\"].astype(str).tolist()\n",
    "\n",
    "            self.output1 = torch.tensor(mpnet.encode(texts1, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            self.output2 = torch.tensor(mpnet.encode(texts2, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            \n",
    "            self.labels = df[\"label\"]\n",
    "\n",
    "            torch.save(self.output1, f\"Cached_MPNet/{name}_output1.pt\")\n",
    "            torch.save(self.output2, f\"Cached_MPNet/{name}_output2.pt\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.output1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        return self.output1[i], self.output2[i], torch.tensor(self.labels.iloc[i], dtype=torch.float)\n",
    "\n",
    "class MPNetTestDataset(Dataset):\n",
    "    def __init__(self, df_name, parameters, reset_cache=False):\n",
    "        # Name is in form <path>/<name>.csv\n",
    "        name = df_name.split(\"/\")[-1]\n",
    "        name = name.split(\".\")[0]\n",
    "        print(name)\n",
    "\n",
    "        df = pd.read_csv(df_name)\n",
    "        \n",
    "        if os.path.exists(f\"Cached_MPNet/{name}_output1.pt\") and os.path.exists(f\"Cached_MPNet/{name}_output2.pt\") and reset_cache == False:\n",
    "            print(\"Loading cached MPNet outputs...\")\n",
    "            loaded_o1 = torch.load(f\"Cached_MPNet/{name}_output1.pt\", weights_only=True)\n",
    "            self.output1 = torch.stack([torch.tensor(t) for t in loaded_o1])\n",
    "            self.output2 = torch.stack([torch.tensor(t) for t in torch.load(f\"Cached_MPNet/{name}_output2.pt\", weights_only=True)])\n",
    "            print(f\"Successfully loaded {len(self.output1)} MPNet outputs\")\n",
    "\n",
    "        else:\n",
    "            mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "            texts1 = df[\"text_1\"].astype(str).tolist()\n",
    "            texts2 = df[\"text_2\"].astype(str).tolist()\n",
    "\n",
    "            self.output1 = torch.tensor(mpnet.encode(texts1, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            self.output2 = torch.tensor(mpnet.encode(texts2, batch_size=16, convert_to_tensor=True, show_progress_bar=True))\n",
    "            \n",
    "            torch.save(self.output1, f\"Cached_MPNet/{name}_output1.pt\")\n",
    "            torch.save(self.output2, f\"Cached_MPNet/{name}_output2.pt\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.output1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        return self.output1[i], self.output2[i]\n",
    "\n",
    "# dataset = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=False) #TODO try parameters\n",
    "# val_dataset = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=True)\n",
    "# test_dataset = reformattedDataset(\"Data/AV_trial.csv\", parameters={}, reset_cache=True)\n",
    "# dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our architecture\n",
    "\n",
    "class classifierMPNet(nn.Module):\n",
    "    def __init__(self,child_num):\n",
    "        super(classifierMPNet, self).__init__()\n",
    "\n",
    "        self.child_num = child_num\n",
    "\n",
    "        # add our custom binary classifier layer to end\n",
    "        self.fc1 = nn.Linear(768*4 + 1, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            self.fc1,\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.Dropout(0.3),\n",
    "            self.fc2, \n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.3),\n",
    "            self.fc3\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid() #to convert classifier output to probability\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # use pre computed MPNet tensors\n",
    "        cos_sim = nn.functional.cosine_similarity(input1, input2, dim=1).unsqueeze(1)\n",
    "        combined_inputs = torch.cat((input1, input2, torch.abs(input1-input2), input1*input2, cos_sim), dim=1)\n",
    "\n",
    "        # pass result through our appended classifier layers\n",
    "        classifier_output = self.classifier(combined_inputs)\n",
    "        return classifier_output\n",
    "\n",
    "\n",
    "class AverageClassifier(nn.Module):\n",
    "    def __init__(self, children=5):\n",
    "        super(AverageClassifier, self).__init__()\n",
    "        self.clfs = nn.ModuleList([classifierMPNet(child_num) for child_num in range(children)])\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        outs = [clf(input1, input2) for clf in self.clfs]\n",
    "        stack = torch.stack(outs, dim=0)\n",
    "        return torch.mean(stack, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier():\n",
    "    # Hyper-Parameters:\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "    epochs = 20\n",
    "    threshold = 0\n",
    "    weight_decay = 0.01\n",
    "\n",
    "    num_children = 5\n",
    "    model = AverageClassifier(num_children).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    train_ds = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=False)\n",
    "    val_ds = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=False)\n",
    "    val_dl = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # split training dataset into subsets, one for each child\n",
    "    train_ds_splits = torch.utils.data.random_split(train_ds, [len(train_ds) // num_children + (1 if x < len(train_ds) % num_children else 0) for x in range(num_children)])\n",
    "    \n",
    "    # train each child on its own data split\n",
    "    for child_num, child_model in enumerate(model.clfs):\n",
    "        print(f\"Training child #{child_num + 1}\")\n",
    "\n",
    "        optimiser = torch.optim.AdamW(child_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        child_dl = DataLoader(train_ds_splits[child_num], batch_size=batch_size, shuffle=True)\n",
    "        for epoch in range(epochs):\n",
    "            child_model.train()\n",
    "            loss_list = []\n",
    "\n",
    "            for s1, s2, l in child_dl:\n",
    "                s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "\n",
    "                optimiser.zero_grad()\n",
    "                output = child_model(s1, s2).reshape(l.shape[0]) \n",
    "                loss = criterion(output, l)\n",
    "                loss_list.append(loss.detach().cpu().numpy()) \n",
    "                loss.backward() \n",
    "                optimiser.step()\n",
    "            \n",
    "            print(f\"Child: {child_num+1}, Epoch {epoch+1}, Loss: {np.mean(loss_list)}\")\n",
    "\n",
    "        \n",
    "        loss_list = []\n",
    "        preds = []\n",
    "        true_labels = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for s1, s2, l in val_dl:\n",
    "                s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "                output = model(s1, s2).reshape(l.shape[0])\n",
    "                loss = criterion(output, l)\n",
    "                loss_list.append(loss.detach().cpu().numpy())\n",
    "                preds.append(1 if output[0] > threshold else 0)\n",
    "                true_labels.append(l.detach().cpu().numpy())\n",
    "\n",
    "        correct = 0\n",
    "        TPs, FPs, FNs, TNs = 0, 0, 0, 0\n",
    "        for j in range(len(true_labels)):\n",
    "            if true_labels[j] == preds[j]:\n",
    "                correct += 1\n",
    "                if true_labels[j] == 1:\n",
    "                    TPs += 1\n",
    "                else:\n",
    "                    TNs += 1\n",
    "            else:\n",
    "                if true_labels[j] == 1:\n",
    "                    FNs += 1\n",
    "                else:\n",
    "                    FPs += 1\n",
    "\n",
    "        accuracy = correct / len(true_labels)\n",
    "        recall = 0 if TPs + FNs == 0 else TPs / (TPs + FNs)\n",
    "        precision = 0 if TPs + FPs == 0 else TPs / (TPs + FPs)\n",
    "        f1 = 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        print(f\"Testing Stats for Epoch {epoch+1}\")\n",
    "        print(f\"    Mean Loss: {np.mean(loss_list)}\")\n",
    "        print(f\"    Accuracy: {accuracy}\")\n",
    "        print(f\"    _____________________________\")\n",
    "        print(f\"    |True\\Pred|Positive|Negative|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    |Positive |TPs: {TPs}|FNs: {FNs}|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    |Negative |FPs: {FPs}|TNs: {TNs}|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    \")\n",
    "        print(f\"    Precision: {precision}\")\n",
    "        print(f\"    Recall: {recall}\")\n",
    "        print(f\"    F1 Score: {f1}\")\n",
    "\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(model):\n",
    "    val_ds = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=False)\n",
    "    val_dl = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    threshold = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for s1, s2, l in val_dl:\n",
    "            s1, s2, l = s1.to(device), s2.to(device), l.to(device)\n",
    "            output = model(s1, s2).reshape(l.shape[0])\n",
    "            loss = criterion(output, l)\n",
    "            loss_list.append(loss.detach().cpu().numpy())\n",
    "            preds.append(1 if output[0] > threshold else 0)\n",
    "            true_labels.append(l.detach().cpu().numpy())\n",
    "\n",
    "    correct = 0\n",
    "    TPs, FPs, FNs, TNs = 0, 0, 0, 0\n",
    "    for j in range(len(true_labels)):\n",
    "        if true_labels[j] == preds[j]:\n",
    "            correct += 1\n",
    "            if true_labels[j] == 1:\n",
    "                TPs += 1\n",
    "            else:\n",
    "                TNs += 1\n",
    "        else:\n",
    "            if true_labels[j] == 1:\n",
    "                FNs += 1\n",
    "            else:\n",
    "                FPs += 1\n",
    "\n",
    "    accuracy = correct / len(true_labels)\n",
    "    recall = 0 if TPs + FNs == 0 else TPs / (TPs + FNs)\n",
    "    precision = 0 if TPs + FPs == 0 else TPs / (TPs + FPs)\n",
    "    f1 = 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Testing Stats for MPNet Classifer (Threshold: {threshold})\")\n",
    "    print(f\"    Mean Loss: {np.mean(loss_list)}\")\n",
    "    print(f\"    Accuracy: {accuracy}\")\n",
    "    print(f\"    _____________________________\")\n",
    "    print(f\"    |True\\Pred|Positive|Negative|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    |Positive |TPs: {TPs}|FNs: {FNs}|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    |Negative |FPs: {FPs}|TNs: {TNs}|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    \")\n",
    "    print(f\"    Precision: {precision}\")\n",
    "    print(f\"    Recall: {recall}\")\n",
    "    print(f\"    F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model, data_path):\n",
    "    test_ds = MPNetTestDataset(data_path, parameters={}, reset_cache=False)\n",
    "    test_dl = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    loss_list = []\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    threshold = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s1, s2 in test_dl:\n",
    "            s1, s2 = s1.to(device), s2.to(device)\n",
    "\n",
    "            output = model(s1, s2)\n",
    "            preds.append(1 if output[0] > threshold else 0)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook \"Engine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook parameters - decides which of the next cells actually run\n",
    "generate_new_encodings = False\n",
    "train_new_classifier = True\n",
    "test_the_classifier = True\n",
    "generate_evaluation_csv = True\n",
    "evaluate_classifier = True\n",
    "\n",
    "test_data_path = \"Data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset - run_next usually set to False as this means you can spam shift+enter to get through notebook\n",
    "if generate_new_encodings:\n",
    "    dataset = reformattedDataset(\"Data/train.csv\", parameters={}, reset_cache=True) #TODO try parameters\n",
    "    val_dataset = reformattedDataset(\"Data/dev.csv\", parameters={}, reset_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 27643 MPNet outputs\n",
      "dev\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 5993 MPNet outputs\n",
      "Training child #1\n",
      "Child: 1, Epoch 1, Loss: 0.7315905094146729\n",
      "Child: 1, Epoch 2, Loss: 0.6939437985420227\n",
      "Child: 1, Epoch 3, Loss: 0.6767875552177429\n",
      "Child: 1, Epoch 4, Loss: 0.648622989654541\n",
      "Child: 1, Epoch 5, Loss: 0.6158220171928406\n",
      "Child: 1, Epoch 6, Loss: 0.5810678005218506\n",
      "Child: 1, Epoch 7, Loss: 0.5512688159942627\n",
      "Child: 1, Epoch 8, Loss: 0.5111411809921265\n",
      "Child: 1, Epoch 9, Loss: 0.4614650309085846\n",
      "Child: 1, Epoch 10, Loss: 0.40178054571151733\n",
      "Child: 1, Epoch 11, Loss: 0.3665044605731964\n",
      "Child: 1, Epoch 12, Loss: 0.2879815101623535\n",
      "Child: 1, Epoch 13, Loss: 0.27258065342903137\n",
      "Child: 1, Epoch 14, Loss: 0.22490130364894867\n",
      "Child: 1, Epoch 15, Loss: 0.20288769900798798\n",
      "Child: 1, Epoch 16, Loss: 0.1624821573495865\n",
      "Child: 1, Epoch 17, Loss: 0.12983304262161255\n",
      "Child: 1, Epoch 18, Loss: 0.10724133253097534\n",
      "Child: 1, Epoch 19, Loss: 0.11020225286483765\n",
      "Child: 1, Epoch 20, Loss: 0.08248675614595413\n",
      "Testing Stats for Epoch 20\n",
      "    Mean Loss: 0.6967225074768066\n",
      "    Accuracy: 0.5831803771066244\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1339|FNs: 1717|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 781|TNs: 2156|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6316037735849057\n",
      "    Recall: 0.4381544502617801\n",
      "    F1 Score: 0.517387944358578\n",
      "Training child #2\n",
      "Child: 2, Epoch 1, Loss: 0.7285398244857788\n",
      "Child: 2, Epoch 2, Loss: 0.6846927404403687\n",
      "Child: 2, Epoch 3, Loss: 0.6605914235115051\n",
      "Child: 2, Epoch 4, Loss: 0.624555766582489\n",
      "Child: 2, Epoch 5, Loss: 0.5868688821792603\n",
      "Child: 2, Epoch 6, Loss: 0.5550117492675781\n",
      "Child: 2, Epoch 7, Loss: 0.5212376713752747\n",
      "Child: 2, Epoch 8, Loss: 0.49012327194213867\n",
      "Child: 2, Epoch 9, Loss: 0.4489721953868866\n",
      "Child: 2, Epoch 10, Loss: 0.3807837963104248\n",
      "Child: 2, Epoch 11, Loss: 0.3474633991718292\n",
      "Child: 2, Epoch 12, Loss: 0.29884254932403564\n",
      "Child: 2, Epoch 13, Loss: 0.2438158541917801\n",
      "Child: 2, Epoch 14, Loss: 0.21192847192287445\n",
      "Child: 2, Epoch 15, Loss: 0.18847329914569855\n",
      "Child: 2, Epoch 16, Loss: 0.15594911575317383\n",
      "Child: 2, Epoch 17, Loss: 0.12786729633808136\n",
      "Child: 2, Epoch 18, Loss: 0.11873113363981247\n",
      "Child: 2, Epoch 19, Loss: 0.11446775496006012\n",
      "Child: 2, Epoch 20, Loss: 0.092784583568573\n",
      "Testing Stats for Epoch 20\n",
      "    Mean Loss: 0.7286517024040222\n",
      "    Accuracy: 0.6117136659436009\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1801|FNs: 1255|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1072|TNs: 1865|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6268708666898712\n",
      "    Recall: 0.5893324607329843\n",
      "    F1 Score: 0.607522347782088\n",
      "Training child #3\n",
      "Child: 3, Epoch 1, Loss: 0.7495554685592651\n",
      "Child: 3, Epoch 2, Loss: 0.691845178604126\n",
      "Child: 3, Epoch 3, Loss: 0.6767779588699341\n",
      "Child: 3, Epoch 4, Loss: 0.6451267600059509\n",
      "Child: 3, Epoch 5, Loss: 0.6087303757667542\n",
      "Child: 3, Epoch 6, Loss: 0.580435037612915\n",
      "Child: 3, Epoch 7, Loss: 0.5486218929290771\n",
      "Child: 3, Epoch 8, Loss: 0.5046601891517639\n",
      "Child: 3, Epoch 9, Loss: 0.4529406726360321\n",
      "Child: 3, Epoch 10, Loss: 0.40130895376205444\n",
      "Child: 3, Epoch 11, Loss: 0.3593542277812958\n",
      "Child: 3, Epoch 12, Loss: 0.3014489710330963\n",
      "Child: 3, Epoch 13, Loss: 0.2491110861301422\n",
      "Child: 3, Epoch 14, Loss: 0.22523657977581024\n",
      "Child: 3, Epoch 15, Loss: 0.20177440345287323\n",
      "Child: 3, Epoch 16, Loss: 0.17014244198799133\n",
      "Child: 3, Epoch 17, Loss: 0.1522098034620285\n",
      "Child: 3, Epoch 18, Loss: 0.11320086568593979\n",
      "Child: 3, Epoch 19, Loss: 0.12196674197912216\n",
      "Child: 3, Epoch 20, Loss: 0.09713179618120193\n",
      "Testing Stats for Epoch 20\n",
      "    Mean Loss: 0.7696865200996399\n",
      "    Accuracy: 0.6222259302519606\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1927|FNs: 1129|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1135|TNs: 1802|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6293272370999347\n",
      "    Recall: 0.6305628272251309\n",
      "    F1 Score: 0.629944426283099\n",
      "Training child #4\n",
      "Child: 4, Epoch 1, Loss: 0.7555268406867981\n",
      "Child: 4, Epoch 2, Loss: 0.6929515600204468\n",
      "Child: 4, Epoch 3, Loss: 0.6903578639030457\n",
      "Child: 4, Epoch 4, Loss: 0.6638888120651245\n",
      "Child: 4, Epoch 5, Loss: 0.62405925989151\n",
      "Child: 4, Epoch 6, Loss: 0.5935419797897339\n",
      "Child: 4, Epoch 7, Loss: 0.5512221455574036\n",
      "Child: 4, Epoch 8, Loss: 0.5104746222496033\n",
      "Child: 4, Epoch 9, Loss: 0.47445401549339294\n",
      "Child: 4, Epoch 10, Loss: 0.40934380888938904\n",
      "Child: 4, Epoch 11, Loss: 0.3725775480270386\n",
      "Child: 4, Epoch 12, Loss: 0.3092649281024933\n",
      "Child: 4, Epoch 13, Loss: 0.27974459528923035\n",
      "Child: 4, Epoch 14, Loss: 0.24179551005363464\n",
      "Child: 4, Epoch 15, Loss: 0.2023979127407074\n",
      "Child: 4, Epoch 16, Loss: 0.186536505818367\n",
      "Child: 4, Epoch 17, Loss: 0.14319400489330292\n",
      "Child: 4, Epoch 18, Loss: 0.10991551727056503\n",
      "Child: 4, Epoch 19, Loss: 0.12069535255432129\n",
      "Child: 4, Epoch 20, Loss: 0.09743607044219971\n",
      "Testing Stats for Epoch 20\n",
      "    Mean Loss: 0.7983638644218445\n",
      "    Accuracy: 0.6319038878691807\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1966|FNs: 1090|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1116|TNs: 1821|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6378974691758599\n",
      "    Recall: 0.643324607329843\n",
      "    F1 Score: 0.6405995438253504\n",
      "Training child #5\n",
      "Child: 5, Epoch 1, Loss: 0.7363367080688477\n",
      "Child: 5, Epoch 2, Loss: 0.6925613880157471\n",
      "Child: 5, Epoch 3, Loss: 0.6726288795471191\n",
      "Child: 5, Epoch 4, Loss: 0.6436201930046082\n",
      "Child: 5, Epoch 5, Loss: 0.6021283864974976\n",
      "Child: 5, Epoch 6, Loss: 0.5782225728034973\n",
      "Child: 5, Epoch 7, Loss: 0.5418726205825806\n",
      "Child: 5, Epoch 8, Loss: 0.4906874895095825\n",
      "Child: 5, Epoch 9, Loss: 0.4355561137199402\n",
      "Child: 5, Epoch 10, Loss: 0.38710159063339233\n",
      "Child: 5, Epoch 11, Loss: 0.334459513425827\n",
      "Child: 5, Epoch 12, Loss: 0.29868757724761963\n",
      "Child: 5, Epoch 13, Loss: 0.236024871468544\n",
      "Child: 5, Epoch 14, Loss: 0.2021722048521042\n",
      "Child: 5, Epoch 15, Loss: 0.159992516040802\n",
      "Child: 5, Epoch 16, Loss: 0.14479459822177887\n",
      "Child: 5, Epoch 17, Loss: 0.15502186119556427\n",
      "Child: 5, Epoch 18, Loss: 0.13107143342494965\n",
      "Child: 5, Epoch 19, Loss: 0.09271904826164246\n",
      "Child: 5, Epoch 20, Loss: 0.09673106670379639\n",
      "Testing Stats for Epoch 20\n",
      "    Mean Loss: 0.8982014060020447\n",
      "    Accuracy: 0.6395795094276656\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1991|FNs: 1065|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1095|TNs: 1842|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6451717433570966\n",
      "    Recall: 0.6515052356020943\n",
      "    F1 Score: 0.6483230218169977\n"
     ]
    }
   ],
   "source": [
    "if train_new_classifier:\n",
    "    mpnet_classifier = train_classifier()\n",
    "    torch.save(mpnet_classifier.state_dict(), \"Cached_MPNet/trained_classifier_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 5993 MPNet outputs\n",
      "Testing Stats for MPNet Classifer (Threshold: 0)\n",
      "    Mean Loss: 0.8982014060020447\n",
      "    Accuracy: 0.6395795094276656\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1991|FNs: 1065|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1095|TNs: 1842|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6451717433570966\n",
      "    Recall: 0.6515052356020943\n",
      "    F1 Score: 0.6483230218169977\n"
     ]
    }
   ],
   "source": [
    "if evaluate_classifier:\n",
    "    mpnet_classifier = AverageClassifier(children=5).to(device)\n",
    "    mpnet_classifier.load_state_dict(torch.load(\"Cached_MPNet/trained_classifier_weights.pt\", weights_only=True))\n",
    "    eval_classifier(mpnet_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 5985 MPNet outputs\n"
     ]
    }
   ],
   "source": [
    "if test_the_classifier:\n",
    "    mpnet_classifier = AverageClassifier().to(device)\n",
    "    mpnet_classifier.load_state_dict(torch.load(\"Cached_MPNet/trained_classifier_weights.pt\", weights_only=True))\n",
    "    predictions = test_classifier(mpnet_classifier, test_data_path)\n",
    "    preds_df = pd.DataFrame(predictions)\n",
    "    preds_df.to_csv(\"Group_17_C.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Loading cached MPNet outputs...\n",
      "Successfully loaded 5985 MPNet outputs\n",
      "Done Test\n"
     ]
    }
   ],
   "source": [
    "if generate_evaluation_csv:\n",
    "    mpnet_classifier = AverageClassifier().to(device)\n",
    "    mpnet_classifier.load_state_dict(torch.load(\"Cached_MPNet/trained_classifier_weights.pt\", weights_only=True))\n",
    "    predictions = test_classifier(mpnet_classifier, \"Data/dev.csv\")\n",
    "    print(\"Done Test\")\n",
    "    predictions = [\"prediction\"] + predictions\n",
    "    preds_df = pd.DataFrame(predictions)\n",
    "    preds_df.to_csv(\"MPNet_Eval.csv\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
