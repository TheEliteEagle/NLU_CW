{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details of solution 1 implementation\n",
    "\n",
    "Unsupervised Learning Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "import spellchecker.spellchecker as spellchecker\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Binary Classifier\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, transform = None):\n",
    "        self.fv_size = (len(data.columns) - 1) // 2\n",
    "        self.data = self.convert_to_tensors(data)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.data[idx][0], self.data[idx][1]\n",
    "\n",
    "    def convert_to_tensors(self, data: pd.DataFrame) -> list[list[Tensor]]:\n",
    "        feature_vecs = torch.tensor(data.iloc[:, :-1].values, dtype=torch.float)\n",
    "        labels = torch.tensor(data.iloc[:, -1].values, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        ret_list = []\n",
    "        for i in range(len(data)):\n",
    "            ret_list.append([feature_vecs[i], labels[i]])\n",
    "        return ret_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vector Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punct_info(words: list[str], text: str, verbose: bool):\n",
    "    all_puncts = string.punctuation\n",
    "    simple_puncts = [\",\", \".\", \"!\", \"?\", '\"', \"'\"]\n",
    "\n",
    "    punct_count = 0\n",
    "    simple_punct_count = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in all_puncts:\n",
    "            punct_count += 1\n",
    "        if word in simple_puncts:\n",
    "            simple_punct_count += 1\n",
    "\n",
    "    complex_punct_count = punct_count - simple_punct_count\n",
    "\n",
    "    punct_complexity = complex_punct_count / punct_count if punct_count != 0 else 0\n",
    "\n",
    "    punct_dist = np.zeros(len(all_puncts))\n",
    "    for char in all_puncts:\n",
    "        punct_dist[all_puncts.index(char)] = text.count(char)\n",
    "\n",
    "    punct_dist = np.zeros(len(all_puncts)) if sum(punct_dist) == 0 else punct_dist / sum(punct_dist)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Punctuation count: {punct_count}\")\n",
    "        print(f\"Punctuation complexity: {punct_complexity}\")\n",
    "        print(f\"Punctuation distribution: {punct_dist}\")\n",
    "    \n",
    "    return punct_count, punct_complexity, punct_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dt_nn(tagged, verbose: bool = False):\n",
    "    det_count = 0\n",
    "    nn_count = 0\n",
    "\n",
    "    for word in tagged:\n",
    "        if word[1] == \"DET\":\n",
    "            det_count += 1\n",
    "        if word[1] == \"NOUN\":\n",
    "            nn_count += 1\n",
    "\n",
    "    dt_nn_ratio = 0 if nn_count == 0 else det_count / nn_count\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"DT count: {det_count}\")\n",
    "        print(f\"NN count: {nn_count}\")\n",
    "        print(f\"DT:NN ratio: {dt_nn_ratio}\")\n",
    "\n",
    "    return dt_nn_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_len_features(words: list[str], verbose: bool = False):\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    \n",
    "    word_len_range = max(word_lengths) - min(word_lengths)\n",
    "    word_len_q3 = np.percentile(word_lengths, 75)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Word length range: {word_len_range}\")\n",
    "        print(f\"Word length Q3: {word_len_q3}\")\n",
    "\n",
    "    return word_len_range, word_len_q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_len_features(sents: list[str], verbose: bool = False):\n",
    "    sent_lengths = [len(nltk.word_tokenize(sent)) for sent in sents]\n",
    "    \n",
    "    sent_len_range = max(sent_lengths) - min(sent_lengths)\n",
    "    sent_len_q3 = np.percentile(sent_lengths, 75)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Sentence length range: {sent_len_range}\")\n",
    "        print(f\"Sentence length Q3: {sent_len_q3}\")\n",
    "\n",
    "    return sent_len_range, sent_len_q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_caps(tagged, sents: list[str], verbose: bool = False):\n",
    "    proper_nouns = [word for word in tagged if word[1] == \"NNP\"]\n",
    "    correct_caps_prop_nouns = 0\n",
    "\n",
    "    for word in proper_nouns:\n",
    "        if word[0][0].isupper():\n",
    "            correct_caps_prop_nouns += 1\n",
    "\n",
    "    prop_noun_cap_ratio = 0 if len(proper_nouns) == 0 else correct_caps_prop_nouns / len(proper_nouns)\n",
    "\n",
    "    correct_caps_sos = 0\n",
    "    for sent in sents:\n",
    "        if sent[0][0].isupper():\n",
    "            correct_caps_sos = 0\n",
    "\n",
    "    sos_cap_ratio = correct_caps_sos / len(sents)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Proper noun caps ratio: {prop_noun_cap_ratio}\")\n",
    "        print(f\"SOS capitalisation ratio: {sos_cap_ratio}\")\n",
    "\n",
    "    return prop_noun_cap_ratio, sos_cap_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_typo_stats(words: list[str], verbose: bool = False):\n",
    "    spell = spellchecker.SpellChecker(\"en\")\n",
    "    misspelled = spell.unknown(words)\n",
    "    typo_ratio = len(misspelled) / len(words)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Typo ratio: {typo_ratio}\")\n",
    "\n",
    "    return typo_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_token(words: list[str], verbose: bool = False):\n",
    "    type_token_ratio = len(set(words)) / len(words)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Type-Token Ratio: {type_token_ratio}\")\n",
    "\n",
    "    return type_token_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readability(text: str, verbose: bool = False):\n",
    "    readability = textstat.textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Reading grade: {reability}\")\n",
    "\n",
    "    return readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Feature Vector Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(text: str, verbose: bool = False) -> np.ndarray:\n",
    "    words = text.split()\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    tagged_uni = nltk.pos_tag(words, tagset=\"universal\")\n",
    "    tagged_reg = nltk.pos_tag(words)\n",
    "\n",
    "    feat_vec = np.ndarray(0, dtype=np.float32)\n",
    "\n",
    "    punc_count, punct_complexity, punct_dist = get_punct_info(words, text, verbose)\n",
    "\n",
    "    dt_nn_ratio = get_dt_nn(tagged_uni, verbose)\n",
    "\n",
    "    word_len_range, word_len_q3 = get_word_len_features(words, verbose)\n",
    "\n",
    "    sent_len_range, sent_len_q3 = get_sent_len_features(sents, verbose)\n",
    "\n",
    "    prop_noun_cap_ratio, sos_cap_ratio = check_caps(tagged_reg, sents, verbose)\n",
    "\n",
    "    typo_ratio = get_typo_stats(words, verbose)\n",
    "\n",
    "    type_token_ratio = get_type_token(words, verbose)\n",
    "\n",
    "    readability = get_readability(text, verbose)\n",
    "    \n",
    "    feat_vec = np.append(feat_vec, punct_dist)\n",
    "    feat_vec = np.append(feat_vec, punct_complexity)\n",
    "    feat_vec = np.append(feat_vec, dt_nn_ratio)\n",
    "    feat_vec = np.append(feat_vec, word_len_range)\n",
    "    feat_vec = np.append(feat_vec, word_len_q3)\n",
    "    feat_vec = np.append(feat_vec, sent_len_range)\n",
    "    feat_vec = np.append(feat_vec, sent_len_q3)\n",
    "    feat_vec = np.append(feat_vec, prop_noun_cap_ratio)\n",
    "    feat_vec = np.append(feat_vec, sos_cap_ratio)\n",
    "    feat_vec = np.append(feat_vec, typo_ratio)\n",
    "    feat_vec = np.append(feat_vec, type_token_ratio)\n",
    "    feat_vec = np.append(feat_vec, readability)\n",
    "\n",
    "    return feat_vec.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vecs(file_path, name):\n",
    "    feat_vec_size = 43\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    col_names = [f\"fv_1_{i}\" for i in range(feat_vec_size)] + [f\"fv_2_{i}\" for i in range(feat_vec_size)] + [f\"fv_diff_{i}\" for i in range(feat_vec_size)] + [\"label\"]\n",
    "\n",
    "    feat_vec_df = pd.DataFrame(columns=col_names)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"\\rProcessing pair {i}/{len(df)}\", end=\"\")\n",
    "\n",
    "        label = df[\"label\"]\n",
    "\n",
    "        s1_fv = get_feature_vector(df[\"text_1\"][i]).flatten()\n",
    "        s2_fv = get_feature_vector(df[\"text_2\"][i]).flatten()\n",
    "        fv_diff = np.abs(s1_fv - s2_fv)\n",
    "\n",
    "        row = np.concatenate((s1_fv, s2_fv, fv_diff, [label[i]]), axis=0)\n",
    "        \n",
    "        feat_vec_df = feat_vec_df._append(pd.DataFrame([row], columns=col_names), ignore_index=True)\n",
    "\n",
    "    feat_vec_df.to_csv(f\"{name}_feature_vectors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat_dataset(df: pd.DataFrame) -> FeatureDataset:\n",
    "    return FeatureDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier():\n",
    "    print(\"Beginning Training\")\n",
    "    feat_vec_train_df = pd.read_csv(\"training_feature_vectors.csv\")\n",
    "    feat_vec_eval_df = pd.read_csv(\"eval_feature_vectors.csv\")\n",
    "    print(feat_vec_train_df.head())\n",
    "    feat_vec_train_ds = get_feat_dataset(feat_vec_train_df)\n",
    "    feat_vec_eval_ds = get_feat_dataset(feat_vec_eval_df)\n",
    "    print(type(feat_vec_train_ds))\n",
    "    print(feat_vec_train_ds[0])\n",
    "    feat_vec_train_dl = DataLoader(feat_vec_train_ds, batch_size=4, shuffle=True)\n",
    "    feat_vec_test_dl = DataLoader(feat_vec_eval_ds, batch_size=1, shuffle=False)\n",
    "    \n",
    "    feat_vec_size = feat_vec_train_df.shape[1] - 1\n",
    "    model = BinaryClassifier(feat_vec_size)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    epochs = 20\n",
    "    threshold = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        loss_list = []\n",
    "        model.train()\n",
    "        for fv, label in feat_vec_train_dl:\n",
    "            optimiser.zero_grad()\n",
    "            output = model(fv)\n",
    "            loss = criterion(output, label)\n",
    "            loss_list.append(loss.detach().numpy())\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        print(f\"Mean Training Loss for Epoch {i}: {np.mean(loss_list)}\")\n",
    "\n",
    "        loss_list = []\n",
    "        preds = []\n",
    "        true_labels = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for fv, label in feat_vec_test_dl:\n",
    "                output = model(fv)\n",
    "                loss = criterion(output, label)\n",
    "                loss_list.append(loss.detach().numpy())\n",
    "                preds.append(1 if output[0] > threshold else 0)\n",
    "                true_labels.append(label.detach().numpy())\n",
    "\n",
    "        correct = 0\n",
    "        TPs, FPs, FNs, TNs = 0, 0, 0, 0\n",
    "        for j in range(len(true_labels)):\n",
    "            if true_labels[j] == preds[j]:\n",
    "                correct += 1\n",
    "                if true_labels[j] == 1:\n",
    "                    TPs += 1\n",
    "                else:\n",
    "                    TNs += 1\n",
    "            else:\n",
    "                if true_labels[j] == 1:\n",
    "                    FNs += 1\n",
    "                else:\n",
    "                    FPs += 1\n",
    "\n",
    "        accuracy = correct / len(true_labels)\n",
    "        recall = 0 if TPs + FNs == 0 else TPs / (TPs + FNs)\n",
    "        precision = 0 if TPs + FPs == 0 else TPs / (TPs + FPs)\n",
    "        f1 = 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        print(f\"Testing Stats for Epoch {i}\")\n",
    "        print(f\"    Mean Loss: {np.mean(loss_list)}\")\n",
    "        print(f\"    Accuracy: {accuracy}\")\n",
    "        print(f\"    _____________________________\")\n",
    "        print(f\"    |True\\Pred|Positive|Negative|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    |Positive |TPs: {TPs}|FNs: {FNs}|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    |Negative |FPs: {FPs}|TNs: {TNs}|\")\n",
    "        print(f\"    |---------|--------|--------|\")\n",
    "        print(f\"    \")\n",
    "        print(f\"    Precision: {precision}\")\n",
    "        print(f\"    Recall: {recall}\")\n",
    "        print(f\"    F1 Score: {f1}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model):\n",
    "    feat_vec_df = pd.read_csv(\"testing_feature_vectors.csv\")\n",
    "    feat_vec_ds = get_feat_test_dataset(feat_vec_df)\n",
    "    feat_vec_dl = DataLoader(feat_vec_ds, batch_size=1, shuffle=False)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    loss_list = []\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    threshold = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for fv, label in feat_vec_dl:\n",
    "            output = model(fv)\n",
    "            loss = criterion(output, label)\n",
    "            loss_list.append(loss.detach().numpy())\n",
    "            preds.append(1 if output[0] > threshold else 0)\n",
    "            true_labels.append(label.detach().numpy())\n",
    "    \n",
    "    correct = 0\n",
    "    TPs = 0\n",
    "    FPs = 0\n",
    "    TNs = 0\n",
    "    FNs = 0\n",
    "\n",
    "    for i in range(len(true_labels)):\n",
    "        if true_labels[i] == preds[i]:\n",
    "            correct += 1\n",
    "            if true_labels[i] == 1:\n",
    "                TPs += 1\n",
    "            else:\n",
    "                TNs += 1\n",
    "        else:\n",
    "            if true_labels[i] == 1:\n",
    "                FNs += 1\n",
    "            else:\n",
    "                FPs += 1\n",
    "\n",
    "    accuracy = correct / len(true_labels)\n",
    "    recall = 0 if TPs + FNs == 0 else TPs / (TPs + FNs)\n",
    "    precision = 0 if TPs + FPs == 0 else TPs / (TPs + FPs)\n",
    "    f1 = 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Testing Stats for Classifier\")\n",
    "    print(f\"    Mean Loss: {np.mean(loss_list)}\")\n",
    "    print(f\"    Accuracy: {accuracy}\")\n",
    "    print(f\"    _____________________________\")\n",
    "    print(f\"    |True\\Pred|Positive|Negative|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    |Positive |TPs: {TPs}|FNs: {FNs}|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    |Negative |FPs: {FPs}|TNs: {TNs}|\")\n",
    "    print(f\"    |---------|--------|--------|\")\n",
    "    print(f\"    \")\n",
    "    print(f\"    Precision: {precision}\")\n",
    "    print(f\"    Recall: {recall}\")\n",
    "    print(f\"    F1 Score: {f1}\")\n",
    "\n",
    "    return (accuracy, recall, precision, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook \"Engine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Mode Switch\n",
    "generate_new_feat_vecs = False\n",
    "inference_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training\n",
      "   fv_1_0  fv_1_1  fv_1_2  fv_1_3  fv_1_4  fv_1_5    fv_1_6  fv_1_7  fv_1_8  \\\n",
      "0     0.0     0.0     0.0     0.0     0.0     0.0  0.000000     0.0     0.2   \n",
      "1     0.1     0.0     0.0     0.0     0.0     0.0  0.000000     0.0     0.0   \n",
      "2     0.0     0.0     0.0     0.0     0.0     0.0  0.333333     0.0     0.0   \n",
      "3     0.0     0.0     0.0     0.0     0.0     0.0  0.238095     0.0     0.0   \n",
      "4     0.0     0.0     0.0     0.0     0.0     0.0  0.333333     0.0     0.0   \n",
      "\n",
      "   fv_1_9  ...  fv_diff_34  fv_diff_35  fv_diff_36  fv_diff_37  fv_diff_38  \\\n",
      "0     0.0  ...         3.0        2.00         1.0        6.25    0.944444   \n",
      "1     0.0  ...        29.0       12.00        34.0       13.50    1.000000   \n",
      "2     0.0  ...         0.0        4.25        11.0        1.00    0.000000   \n",
      "3     0.0  ...         2.0        0.00        28.0       12.25    0.000000   \n",
      "4     0.0  ...         6.0        1.75        33.0       19.25    0.120000   \n",
      "\n",
      "   fv_diff_39  fv_diff_40  fv_diff_41  fv_diff_42  label  \n",
      "0         0.0    0.041667    0.041667         0.4    0.0  \n",
      "1         0.0    0.574242    0.224242        16.3    0.0  \n",
      "2         0.0    0.040404    0.090909         5.3    1.0  \n",
      "3         0.0    0.015008    0.010410         3.6    0.0  \n",
      "4         0.0    0.107632    0.246575         5.0    1.0  \n",
      "\n",
      "[5 rows x 130 columns]\n",
      "<class '__main__.FeatureDataset'>\n",
      "(tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.2000,  0.0000,  0.0000,  0.0000,  0.4000,  0.2000,  0.0000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         1.0000,  0.0000,  8.0000,  4.0000, 13.0000, 11.7500,  0.0000,  0.0000,\n",
      "         0.1333,  0.9333,  4.4000,  0.0455,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0455,  0.0909,  0.0455,  0.0455,  0.0000,  0.0000,  0.0455,  0.1818,\n",
      "         0.4545,  0.0000,  0.0455,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  1.0000,  0.0000, 11.0000,  6.0000, 12.0000,\n",
      "         5.5000,  0.9444,  0.0000,  0.1750,  0.9750,  4.8000,  0.0455,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0455,  0.0909,  0.0455,  0.1545,  0.0000,\n",
      "         0.0000,  0.0455,  0.2182,  0.2545,  0.0000,  0.1545,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         3.0000,  2.0000,  1.0000,  6.2500,  0.9444,  0.0000,  0.0417,  0.0417,\n",
      "         0.4000]), tensor([0.]))\n",
      "Mean Training Loss for Epoch 0: 0.6968677639961243\n",
      "Testing Stats for Epoch 0\n",
      "    Mean Loss: 0.6860647201538086\n",
      "    Accuracy: 0.5601535124311697\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1587|FNs: 1469|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1167|TNs: 1770|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.5762527233115469\n",
      "    Recall: 0.5193062827225131\n",
      "    F1 Score: 0.5462994836488813\n",
      "Mean Training Loss for Epoch 1: 0.6890658736228943\n",
      "Testing Stats for Epoch 1\n",
      "    Mean Loss: 0.6915579438209534\n",
      "    Accuracy: 0.5453028533288837\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 962|FNs: 2094|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 631|TNs: 2306|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6038920276208412\n",
      "    Recall: 0.3147905759162304\n",
      "    F1 Score: 0.41385244138524413\n",
      "Mean Training Loss for Epoch 2: 0.6856632828712463\n",
      "Testing Stats for Epoch 2\n",
      "    Mean Loss: 0.6802544593811035\n",
      "    Accuracy: 0.5679959953278825\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1550|FNs: 1506|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1083|TNs: 1854|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.5886821116597037\n",
      "    Recall: 0.5071989528795812\n",
      "    F1 Score: 0.544911232202496\n",
      "Mean Training Loss for Epoch 3: 0.6795666813850403\n",
      "Testing Stats for Epoch 3\n",
      "    Mean Loss: 0.6847386956214905\n",
      "    Accuracy: 0.5616552644752211\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1286|FNs: 1770|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 857|TNs: 2080|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6000933271115259\n",
      "    Recall: 0.42081151832460734\n",
      "    F1 Score: 0.4947105212540873\n",
      "Mean Training Loss for Epoch 4: 0.6746692657470703\n",
      "Testing Stats for Epoch 4\n",
      "    Mean Loss: 0.684516966342926\n",
      "    Accuracy: 0.5599866510929418\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 920|FNs: 2136|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 501|TNs: 2436|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6474313863476425\n",
      "    Recall: 0.3010471204188482\n",
      "    F1 Score: 0.41098950189859285\n",
      "Mean Training Loss for Epoch 5: 0.6724247932434082\n",
      "Testing Stats for Epoch 5\n",
      "    Mean Loss: 0.6719009280204773\n",
      "    Accuracy: 0.5786751209744703\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1623|FNs: 1433|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1092|TNs: 1845|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.5977900552486188\n",
      "    Recall: 0.5310863874345549\n",
      "    F1 Score: 0.5624675099636113\n",
      "Mean Training Loss for Epoch 6: 0.6697195172309875\n",
      "Testing Stats for Epoch 6\n",
      "    Mean Loss: 0.6671251654624939\n",
      "    Accuracy: 0.5825129317537127\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1705|FNs: 1351|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1151|TNs: 1786|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.5969887955182073\n",
      "    Recall: 0.5579188481675392\n",
      "    F1 Score: 0.5767929634641407\n",
      "Mean Training Loss for Epoch 7: 0.6652505397796631\n",
      "Testing Stats for Epoch 7\n",
      "    Mean Loss: 0.6669095754623413\n",
      "    Accuracy: 0.5848489904889037\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1791|FNs: 1265|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1223|TNs: 1714|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.5942269409422694\n",
      "    Recall: 0.5860602094240838\n",
      "    F1 Score: 0.5901153212520593\n",
      "Mean Training Loss for Epoch 8: 0.6623693704605103\n",
      "Testing Stats for Epoch 8\n",
      "    Mean Loss: 0.6734312176704407\n",
      "    Accuracy: 0.5770065075921909\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1253|FNs: 1803|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 732|TNs: 2205|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6312342569269521\n",
      "    Recall: 0.4100130890052356\n",
      "    F1 Score: 0.49712358658996236\n",
      "Mean Training Loss for Epoch 9: 0.660435140132904\n",
      "Testing Stats for Epoch 9\n",
      "    Mean Loss: 0.6702494621276855\n",
      "    Accuracy: 0.5920240280327048\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1768|FNs: 1288|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1157|TNs: 1780|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6044444444444445\n",
      "    Recall: 0.5785340314136126\n",
      "    F1 Score: 0.5912054840327704\n",
      "Mean Training Loss for Epoch 10: 0.655617356300354\n",
      "Testing Stats for Epoch 10\n",
      "    Mean Loss: 0.660389244556427\n",
      "    Accuracy: 0.5961955614884031\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1758|FNs: 1298|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1122|TNs: 1815|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6104166666666667\n",
      "    Recall: 0.5752617801047121\n",
      "    F1 Score: 0.5923180592991913\n",
      "Mean Training Loss for Epoch 11: 0.654597282409668\n",
      "Testing Stats for Epoch 11\n",
      "    Mean Loss: 0.6662812232971191\n",
      "    Accuracy: 0.599032204238278\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1748|FNs: 1308|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1095|TNs: 1842|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6148434752022511\n",
      "    Recall: 0.5719895287958116\n",
      "    F1 Score: 0.5926428208170876\n",
      "Mean Training Loss for Epoch 12: 0.6526105403900146\n",
      "Testing Stats for Epoch 12\n",
      "    Mean Loss: 0.6635202169418335\n",
      "    Accuracy: 0.5913565826797931\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 2195|FNs: 861|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1588|TNs: 1349|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.5802273328046524\n",
      "    Recall: 0.7182591623036649\n",
      "    F1 Score: 0.6419067115075303\n",
      "Mean Training Loss for Epoch 13: 0.6522858142852783\n",
      "Testing Stats for Epoch 13\n",
      "    Mean Loss: 0.6634825468063354\n",
      "    Accuracy: 0.5988653429000501\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1615|FNs: 1441|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 963|TNs: 1974|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.626454615981381\n",
      "    Recall: 0.5284685863874345\n",
      "    F1 Score: 0.5733049343272985\n",
      "Mean Training Loss for Epoch 14: 0.6473701000213623\n",
      "Testing Stats for Epoch 14\n",
      "    Mean Loss: 0.6705029010772705\n",
      "    Accuracy: 0.5813449023861171\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1158|FNs: 1898|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 611|TNs: 2326|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6546071226681741\n",
      "    Recall: 0.37892670157068065\n",
      "    F1 Score: 0.48\n",
      "Mean Training Loss for Epoch 15: 0.6484969854354858\n",
      "Testing Stats for Epoch 15\n",
      "    Mean Loss: 0.664479672908783\n",
      "    Accuracy: 0.5868513265476389\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1478|FNs: 1578|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 898|TNs: 2039|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6220538720538721\n",
      "    Recall: 0.4836387434554974\n",
      "    F1 Score: 0.5441826215022092\n",
      "Mean Training Loss for Epoch 16: 0.64629065990448\n",
      "Testing Stats for Epoch 16\n",
      "    Mean Loss: 0.6611394882202148\n",
      "    Accuracy: 0.591523444018021\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 2137|FNs: 919|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1529|TNs: 1408|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.582924168030551\n",
      "    Recall: 0.6992801047120419\n",
      "    F1 Score: 0.6358226718238619\n",
      "Mean Training Loss for Epoch 17: 0.6444607973098755\n",
      "Testing Stats for Epoch 17\n",
      "    Mean Loss: 0.6555042862892151\n",
      "    Accuracy: 0.6003670949441015\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1897|FNs: 1159|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1236|TNs: 1701|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.6054899457389084\n",
      "    Recall: 0.6207460732984293\n",
      "    F1 Score: 0.6130231055097755\n",
      "Mean Training Loss for Epoch 18: 0.6410870552062988\n",
      "Testing Stats for Epoch 18\n",
      "    Mean Loss: 0.6637705564498901\n",
      "    Accuracy: 0.5925246120473886\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 2009|FNs: 1047|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1395|TNs: 1542|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.5901880141010576\n",
      "    Recall: 0.6573952879581152\n",
      "    F1 Score: 0.6219814241486068\n",
      "Mean Training Loss for Epoch 19: 0.6388842463493347\n",
      "Testing Stats for Epoch 19\n",
      "    Mean Loss: 0.6546538472175598\n",
      "    Accuracy: 0.5975304521942266\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 1948|FNs: 1108|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 1304|TNs: 1633|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.5990159901599016\n",
      "    Recall: 0.637434554973822\n",
      "    F1 Score: 0.6176284083703233\n",
      "Testing Stats for Classifier\n",
      "    Mean Loss: 0.6840536594390869\n",
      "    Accuracy: 0.56\n",
      "    _____________________________\n",
      "    |True\\Pred|Positive|Negative|\n",
      "    |---------|--------|--------|\n",
      "    |Positive |TPs: 16|FNs: 9|\n",
      "    |---------|--------|--------|\n",
      "    |Negative |FPs: 13|TNs: 12|\n",
      "    |---------|--------|--------|\n",
      "    \n",
      "    Precision: 0.5517241379310345\n",
      "    Recall: 0.64\n",
      "    F1 Score: 0.5925925925925927\n"
     ]
    }
   ],
   "source": [
    "if generate_new_feat_vecs:\n",
    "    create_feature_vecs(\"Data/train.csv\", \"training\")\n",
    "    create_feature_vecs(\"Data/dev.csv\", \"eval\")\n",
    "    create_feature_vecs(\"trial_data/AV_trial.csv\", \"testing\")\n",
    "if not inference_mode:\n",
    "    classifier = train_classifier()\n",
    "    torch.save(classifier, \"solution_1_classifier.pth\")\n",
    "    results = test_classifier(classifier)\n",
    "else:\n",
    "    classifier = torch.load(\"solution_1_classifier.pth\")\n",
    "    results = test_classifier(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
